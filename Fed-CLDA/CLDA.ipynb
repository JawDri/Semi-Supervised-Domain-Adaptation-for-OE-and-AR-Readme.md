{"cells":[{"cell_type":"markdown","metadata":{"id":"f260223a"},"source":["<a id=\"section-one\"></a>\n","## 1 Import Libraries and download data"],"id":"f260223a"},{"cell_type":"markdown","metadata":{"id":"dd9d1294"},"source":["<a id=\"define-device\"></a>\n","### 1.2 Define device"],"id":"dd9d1294"},{"cell_type":"code","execution_count":1,"metadata":{"id":"FqEDEQQJpwtB","executionInfo":{"status":"ok","timestamp":1724059545611,"user_tz":240,"elapsed":18958,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}},"outputId":"271bd060-2712-4423-a014-23082edabc38","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"id":"FqEDEQQJpwtB"},{"cell_type":"markdown","metadata":{"id":"dgKhIfA6koN3"},"source":["#OE 2 unbalanced labels"],"id":"dgKhIfA6koN3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"oaAL6_V_kn1J"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=1\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=1\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=1\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:1045]\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:300]\n","frames = [Data_0, Data_2]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:772]\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:200]\n","frames = [Data_0, Data_2]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","\n","Source_train = Final_Data_S1.iloc[200:,:]\n","Source_test = Final_Data_S1.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","\n","Source_train.to_csv('/content/drive/MyDrive/CLDA/data/Source_train.csv', index=False)\n","Source_test.to_csv('/content/drive/MyDrive/CLDA/data/Source_test.csv', index=False)\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"oaAL6_V_kn1J"},{"cell_type":"markdown","metadata":{"id":"H5kJaPJNi3uc"},"source":["#OE 3 unbalanced labels"],"id":"H5kJaPJNi3uc"},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6s34COBi3MM"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=2\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=2\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=2\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:1045]\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:300]\n","Data_3 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:300]\n","frames = [Data_0, Data_2, Data_3]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:772]\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:200]\n","Data_3 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:200]\n","frames = [Data_0, Data_2, Data_3]\n","Final_Data_S2 = pd.concat(frames)\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","\n","Source_train = Final_Data_S1.iloc[200:,:]\n","Source_test = Final_Data_S1.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","Source_train.to_csv('/content/drive/MyDrive/CLDA/data/Source_train.csv', index=False)\n","Source_test.to_csv('/content/drive/MyDrive/CLDA/data/Source_test.csv', index=False)\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"Y6s34COBi3MM"},{"cell_type":"markdown","metadata":{"id":"iHUKkTdKkJP2"},"source":["#OE balanced 2 labels"],"id":"iHUKkTdKkJP2"},{"cell_type":"code","execution_count":null,"metadata":{"id":"qm7aAMUikGkX"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2020)\n","\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=1\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=1\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=1\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","\n","Source_train = Final_Data_S1.iloc[200:,:]\n","Source_test = Final_Data_S1.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","\n","Source_train.to_csv('/content/drive/MyDrive/CLDA/data/Source_train.csv', index=False)\n","Source_test.to_csv('/content/drive/MyDrive/CLDA/data/Source_test.csv', index=False)\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"qm7aAMUikGkX"},{"cell_type":"markdown","metadata":{"id":"t_6Ru51m_deH"},"source":["#OE 3 labels balanced"],"id":"t_6Ru51m_deH"},{"cell_type":"code","execution_count":null,"metadata":{"id":"La67l8Ng-xmm"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2019)\n","\n","\n","\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S1.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/Final_Data_S2.csv\")\n","\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([3.0, 4.0])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([3.0, 4.0])].index,'labels']=2\n","\n","!pip install imbalanced-learn\n","from imblearn.over_sampling import SMOTE\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin([2.0])].index,'labels']=2\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([0.0])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([1.0])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin([2.0])].index,'labels']=2\n","\n","labels_S1 = Final_Data_S1.labels\n","labels_S2 = Final_Data_S2.labels\n","Data_S1 = Final_Data_S1.drop(['time', 'labels'], axis= 1)\n","Data_S2 = Final_Data_S2.drop(['time', 'labels'], axis= 1)\n","\n","from sklearn.decomposition import PCA\n","pca = PCA(n_components=9)\n","principalComponents_S1 = pca.fit_transform(Data_S1)\n","pca = PCA(n_components=9)\n","principalComponents_S2 = pca.fit_transform(Data_S2)\n","\n","sm = SMOTE(random_state=2)\n","principalComponents_S1, labels_S1 = sm.fit_resample(principalComponents_S1, labels_S1)\n","principalComponents_S2, labels_S2 = sm.fit_resample(principalComponents_S2, labels_S2)\n","\n","Final_Data_S1 = pd.DataFrame(principalComponents_S1)\n","Final_Data_S2 = pd.DataFrame(principalComponents_S2)\n","\n","Final_Data_S1['labels'] = labels_S1\n","Final_Data_S2['labels'] = labels_S2\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","\n","Source_train = Final_Data_S1.iloc[200:,:]\n","Source_test = Final_Data_S1.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","Source_train.to_csv('/content/drive/MyDrive/CLDA/data/Source_train.csv', index=False)\n","Source_test.to_csv('/content/drive/MyDrive/CLDA/data/Source_test.csv', index=False)\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"La67l8Ng-xmm"},{"cell_type":"markdown","metadata":{"id":"aW1uEd59_iOv"},"source":["#Load Data AR: 3 Activities"],"id":"aW1uEd59_iOv"},{"cell_type":"code","execution_count":2,"metadata":{"id":"wyaalwUr-65K","executionInfo":{"status":"ok","timestamp":1724059829008,"user_tz":240,"elapsed":1,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","'''\n","!wget http://casas.wsu.edu/datasets/hh101.zip\n","!wget http://casas.wsu.edu/datasets/hh105.zip\n","!wget http://casas.wsu.edu/datasets/hh104.zip\n","!wget http://casas.wsu.edu/datasets/hh103.zip\n","!unzip '/content/drive/MyDrive/CLDA/hh103.zip'\n","!unzip '/content/drive/MyDrive/CLDA/hh104.zip'\n","!unzip '/content/drive/MyDrive/CLDA/hh105.zip'\n","!unzip /content/drive/MyDrive/CLDA/hh101.zip\n","\n","'''\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh101/hh101.ann.features.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh105/hh105.ann.features.csv\")\n","Final_Data_S3 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh104/hh104.ann.features.csv\")\n","Final_Data_S4 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh103/hh103.ann.features.csv\")\n","\n","Final_Data_S1.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S2.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S3.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S4.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","\n","Final_Data_S1 = Final_Data_S1.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S2 = Final_Data_S2.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","Final_Data_S3 = Final_Data_S3.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S4 = Final_Data_S4.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","\n","Final_Data_S1 = Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Breakfast', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S2 = Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Breakfast', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S3 = Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Breakfast', 'Watch_TV', 'Toilet'])]\n","Final_Data_S4 = Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Breakfast','Watch_TV', 'Toilet' ])]\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Watch_TV'])].index,'labels']=0\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Breakfast'])].index,'labels']=1\n","\n","\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Watch_TV'])].index,'labels']=0\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Breakfast'])].index,'labels']=1\n","\n","\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Watch_TV'])].index,'labels']=0\n","\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Breakfast'])].index,'labels']=1\n","\n","\n","\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Watch_TV'])].index,'labels']=0\n","\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Breakfast'])].index,'labels']=1\n","\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:4000]#2000\n","\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S3[Final_Data_S3.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S3[Final_Data_S3.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S3[Final_Data_S3.labels == 2].iloc[:4000]#2000\n","\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S3 = pd.concat(frames)\n","\n","\n","Data_0 = Final_Data_S4[Final_Data_S4.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S4[Final_Data_S4.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S4[Final_Data_S4.labels == 2].iloc[:4000]#2000\n","\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S4 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:2000]#1000#2000\n","Data_1 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:2000]#2000\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:2000]#\n","\n","frames = [Data_0, Data_1, Data_2]\n","Final_Data_S2 = pd.concat(frames)\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Final_Data_S4 = Final_Data_S4.sample(frac = 1)\n","Final_Data_S3 = Final_Data_S3.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[200:,:]#200#\n","Source_test_1 = Final_Data_S1.iloc[:200,:]\n","\n","Source_train_2 = Final_Data_S4.iloc[200:,:]#200#\n","Source_test_2 = Final_Data_S4.iloc[:200,:]\n","\n","Source_train_3 = Final_Data_S3.iloc[200:,:]#200#\n","Source_test_3 = Final_Data_S3.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_2.csv', index=False)\n","\n","Source_train_3.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_3.csv', index=False)\n","Source_test_3.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_3.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"wyaalwUr-65K"},{"cell_type":"markdown","metadata":{"id":"hRCYevEuAB4A"},"source":["#Load Data AR: 5 Activities"],"id":"hRCYevEuAB4A"},{"cell_type":"code","execution_count":5,"metadata":{"id":"01hqvUJ9AGiO","collapsed":true,"executionInfo":{"status":"ok","timestamp":1724029517084,"user_tz":240,"elapsed":11990,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import random\n","np.random.seed(2021)\n","'''\n","!wget http://casas.wsu.edu/datasets/hh101.zip\n","!wget http://casas.wsu.edu/datasets/hh105.zip\n","!wget http://casas.wsu.edu/datasets/hh104.zip\n","!wget http://casas.wsu.edu/datasets/hh103.zip\n","!unzip '/content/drive/MyDrive/CLDA/hh103.zip'\n","!unzip '/content/drive/MyDrive/CLDA/hh104.zip'\n","!unzip '/content/drive/MyDrive/CLDA/hh105.zip'\n","!unzip /content/drive/MyDrive/CLDA/hh101.zip\n","\n","'''\n","Final_Data_S1 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh101/hh101.ann.features.csv\")\n","Final_Data_S2 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh105/hh105.ann.features.csv\")\n","Final_Data_S3 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh104/hh104.ann.features.csv\")\n","Final_Data_S4 = pd.read_csv(\"/content/drive/MyDrive/CLDA/hh103/hh103.ann.features.csv\")\n","\n","Final_Data_S1.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S2.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S3.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","Final_Data_S4.rename(columns={\"activity\": \"labels\"}, inplace= True)\n","\n","Final_Data_S1 = Final_Data_S1.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S2 = Final_Data_S2.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","Final_Data_S3 = Final_Data_S3.drop(['lastSensorEventHours', 'lastSensorEventSeconds', 'lastSensorDayOfWeek','lastSensorID'], axis= 1)\n","Final_Data_S4 = Final_Data_S4.drop(['lastSensorEventHours', 'lastSensorEventSeconds','lastSensorDayOfWeek', 'lastSensorID'], axis= 1)\n","\n","Final_Data_S1 = Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S2 = Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","Final_Data_S3 = Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet'])]\n","Final_Data_S4 = Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner', 'Cook_Breakfast', 'Cook_Lunch', 'Watch_TV', 'Toilet' ])]\n","\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S1.loc[Final_Data_S1[Final_Data_S1.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S2.loc[Final_Data_S2[Final_Data_S2.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S3.loc[Final_Data_S3[Final_Data_S3.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Watch_TV'])].index,'labels']=0\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Lunch'])].index,'labels']=1\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Toilet'])].index,'labels']=2\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Breakfast'])].index,'labels']=3\n","Final_Data_S4.loc[Final_Data_S4[Final_Data_S4.labels.isin(['Cook_Dinner'])].index,'labels']=4\n","\n","Data_0 = Final_Data_S1[Final_Data_S1.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S1[Final_Data_S1.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S1[Final_Data_S1.labels == 2].iloc[:4000]#2000\n","Data_3 = Final_Data_S1[Final_Data_S1.labels == 3].iloc[:4000]#4000\n","Data_4 = Final_Data_S1[Final_Data_S1.labels == 4].iloc[:4000]#\n","\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S1 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S3[Final_Data_S3.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S3[Final_Data_S3.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S3[Final_Data_S3.labels == 2].iloc[:4000]#2000\n","Data_3 = Final_Data_S3[Final_Data_S3.labels == 3].iloc[:4000]#4000\n","Data_4 = Final_Data_S3[Final_Data_S3.labels == 4].iloc[:4000]#\n","\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S3 = pd.concat(frames)\n","\n","\n","Data_0 = Final_Data_S4[Final_Data_S4.labels == 0].iloc[:4000]#2000#4000\n","Data_1 = Final_Data_S4[Final_Data_S4.labels == 1].iloc[:4000]#4000\n","Data_2 = Final_Data_S4[Final_Data_S4.labels == 2].iloc[:4000]#2000\n","Data_3 = Final_Data_S4[Final_Data_S4.labels == 3].iloc[:4000]#4000\n","Data_4 = Final_Data_S4[Final_Data_S4.labels == 4].iloc[:4000]#\n","\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S4 = pd.concat(frames)\n","\n","Data_0 = Final_Data_S2[Final_Data_S2.labels == 0].iloc[:2000]#1000#2000\n","Data_1 = Final_Data_S2[Final_Data_S2.labels == 1].iloc[:2000]#2000\n","Data_2 = Final_Data_S2[Final_Data_S2.labels == 2].iloc[:2000]#\n","Data_3 = Final_Data_S2[Final_Data_S2.labels == 3].iloc[:2000]#2000\n","Data_4 = Final_Data_S2[Final_Data_S2.labels == 4].iloc[:2000]#\n","frames = [Data_0, Data_1, Data_2, Data_3, Data_4]\n","Final_Data_S2 = pd.concat(frames)\n","\n","\n","Final_Data_S1 = Final_Data_S1.sample(frac = 1)\n","Final_Data_S2 = Final_Data_S2.sample(frac = 1)\n","Final_Data_S4 = Final_Data_S4.sample(frac = 1)\n","Final_Data_S3 = Final_Data_S3.sample(frac = 1)\n","\n","Source_train_1 = Final_Data_S1.iloc[200:,:]#200#\n","Source_test_1 = Final_Data_S1.iloc[:200,:]\n","\n","Source_train_2 = Final_Data_S4.iloc[200:,:]#200#\n","Source_test_2 = Final_Data_S4.iloc[:200,:]\n","\n","Source_train_3 = Final_Data_S3.iloc[200:,:]#200#\n","Source_test_3 = Final_Data_S3.iloc[:200,:]\n","\n","Target_train = Final_Data_S2.iloc[150:250,:]\n","Target_train_unl = Final_Data_S2.iloc[300:,:]\n","Target_val = Final_Data_S2.iloc[250:300,:]\n","\n","Target_test = Final_Data_S2.iloc[:150,:]\n","\n","\n","\n","Target_train_unl.to_csv('/content/drive/MyDrive/CLDA/data/Target_train_unl.csv', index=False)\n","Target_val.to_csv('/content/drive/MyDrive/CLDA/data/Target_val.csv', index=False)\n","\n","Source_train_1.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_1.csv', index=False)\n","Source_test_1.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_1.csv', index=False)\n","\n","Source_train_2.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_2.csv', index=False)\n","Source_test_2.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_2.csv', index=False)\n","\n","Source_train_3.to_csv('/content/drive/MyDrive/CLDA/data/Source_train_3.csv', index=False)\n","Source_test_3.to_csv('/content/drive/MyDrive/CLDA/data/Source_test_3.csv', index=False)\n","\n","\n","Target_train.to_csv('/content/drive/MyDrive/CLDA/data/Target_train.csv', index=False)\n","Target_test.to_csv('/content/drive/MyDrive/CLDA/data/Target_test.csv', index=False)"],"id":"01hqvUJ9AGiO"},{"cell_type":"code","execution_count":3,"metadata":{"id":"rB2HRmYbE8G3","executionInfo":{"status":"ok","timestamp":1724059829008,"user_tz":240,"elapsed":13,"user":{"displayName":"Jawher Dridi","userId":"09677130424740496435"}}},"outputs":[],"source":["import os\n","os.chdir('/content/drive/MyDrive/CLDA/')"],"id":"rB2HRmYbE8G3"},{"cell_type":"code","source":["!python clda_final.py"],"metadata":{"id":"wA3cJwWxcFyB"},"id":"wA3cJwWxcFyB","execution_count":null,"outputs":[]},{"cell_type":"code","source":["!python clda_final_federated.py --balanced 'True'"],"metadata":{"id":"830R2dPrXgCT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e7afd4ac-625d-4d94-ed9b-b2e2ba0b8e09"},"id":"830R2dPrXgCT","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset office_home Source real Target sketch Labeled num perclass 3 Network resnet34\n","3 classes in this dataset\n","./save_model_ssda_office_home_3__real_sketch_20240819-093052\n","Ep: 0 lr: 0.01, loss_all: 20.459808, loss_c: 1.097790, simclr_loss: 4.840505, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 21.508572, loss_c: 1.037922, simclr_loss: 4.842165, grp_loss: 1.101990\n","Test set: Average loss: 1.12, Accuracy: 41/150 F1 (27.33%), F1 score: 23.14%\n","Model saved at step 10 with best accuracy 27.333333333333332\n","Ep: 20 lr: 0.009045084971874737, loss_all: 21.534388, loss_c: 0.640961, simclr_loss: 4.824750, grp_loss: 1.594427\n","Test set: Average loss: 0.93, Accuracy: 107/150 F1 (71.33%), F1 score: 74.51%\n","Model saved at step 20 with best accuracy 71.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.766270, loss_c: 0.200094, simclr_loss: 4.762199, grp_loss: 1.517379\n","Test set: Average loss: 0.63, Accuracy: 94/150 F1 (62.67%), F1 score: 54.66%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.555834, loss_c: 0.098061, simclr_loss: 4.741683, grp_loss: 1.491042\n","Test set: Average loss: 0.62, Accuracy: 93/150 F1 (62.00%), F1 score: 44.00%\n","Ep: 50 lr: 0.005, loss_all: 20.478338, loss_c: 0.049145, simclr_loss: 4.737364, grp_loss: 1.479737\n","Test set: Average loss: 0.48, Accuracy: 101/150 F1 (67.33%), F1 score: 50.36%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.392788, loss_c: 0.021579, simclr_loss: 4.726457, grp_loss: 1.465381\n","Test set: Average loss: 0.39, Accuracy: 129/150 F1 (86.00%), F1 score: 64.84%\n","Model saved at step 60 with best accuracy 86.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.423582, loss_c: 0.049493, simclr_loss: 4.727582, grp_loss: 1.463763\n","Test set: Average loss: 0.18, Accuracy: 138/150 F1 (92.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 92.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.375042, loss_c: 0.020231, simclr_loss: 4.722505, grp_loss: 1.464792\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 96.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.365646, loss_c: 0.018924, simclr_loss: 4.721919, grp_loss: 1.459046\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 96.66666666666667\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 96.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 20.473129, loss_c: 1.111344, simclr_loss: 4.840446, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 21.691332, loss_c: 1.230739, simclr_loss: 4.842583, grp_loss: 1.090261\n","Test set: Average loss: 1.15, Accuracy: 55/150 F1 (36.67%), F1 score: 15.36%\n","Model saved at step 10 with best accuracy 36.666666666666664\n","Ep: 20 lr: 0.009045084971874737, loss_all: 21.017832, loss_c: 0.317524, simclr_loss: 4.786509, grp_loss: 1.554274\n","Test set: Average loss: 0.54, Accuracy: 121/150 F1 (80.67%), F1 score: 91.06%\n","Model saved at step 20 with best accuracy 80.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.573910, loss_c: 0.134293, simclr_loss: 4.736696, grp_loss: 1.492834\n","Test set: Average loss: 0.68, Accuracy: 115/150 F1 (76.67%), F1 score: 82.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.450859, loss_c: 0.060230, simclr_loss: 4.731358, grp_loss: 1.465196\n","Test set: Average loss: 0.76, Accuracy: 116/150 F1 (77.33%), F1 score: 73.30%\n","Ep: 50 lr: 0.005, loss_all: 20.414202, loss_c: 0.034891, simclr_loss: 4.727774, grp_loss: 1.468214\n","Test set: Average loss: 0.84, Accuracy: 115/150 F1 (76.67%), F1 score: 81.82%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.388195, loss_c: 0.020860, simclr_loss: 4.725760, grp_loss: 1.464295\n","Test set: Average loss: 0.58, Accuracy: 121/150 F1 (80.67%), F1 score: 86.58%\n","Model saved at step 60 with best accuracy 80.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.352999, loss_c: 0.008312, simclr_loss: 4.720578, grp_loss: 1.462373\n","Test set: Average loss: 0.25, Accuracy: 134/150 F1 (89.33%), F1 score: 86.21%\n","Model saved at step 70 with best accuracy 89.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.381927, loss_c: 0.040691, simclr_loss: 4.719794, grp_loss: 1.462058\n","Test set: Average loss: 0.14, Accuracy: 145/150 F1 (96.67%), F1 score: 95.08%\n","Model saved at step 80 with best accuracy 96.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.366337, loss_c: 0.018241, simclr_loss: 4.722457, grp_loss: 1.458265\n","Test set: Average loss: 0.08, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Test set: Average loss: 0.08, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 20.476595, loss_c: 1.113107, simclr_loss: 4.840872, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.329941, loss_c: 0.960027, simclr_loss: 4.842478, grp_loss: 0.000000\n","Test set: Average loss: 0.98, Accuracy: 77/150 F1 (51.33%), F1 score: 42.06%\n","Model saved at step 10 with best accuracy 51.333333333333336\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.717548, loss_c: 0.521039, simclr_loss: 4.789855, grp_loss: 1.037092\n","Test set: Average loss: 0.74, Accuracy: 96/150 F1 (64.00%), F1 score: 49.55%\n","Model saved at step 20 with best accuracy 64.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.336817, loss_c: 0.215509, simclr_loss: 4.776865, grp_loss: 1.013848\n","Test set: Average loss: 0.93, Accuracy: 79/150 F1 (52.67%), F1 score: 62.68%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.215355, loss_c: 0.148473, simclr_loss: 4.764900, grp_loss: 1.007281\n","Test set: Average loss: 0.75, Accuracy: 87/150 F1 (58.00%), F1 score: 62.17%\n","Ep: 50 lr: 0.005, loss_all: 20.110575, loss_c: 0.098475, simclr_loss: 4.754390, grp_loss: 0.994542\n","Test set: Average loss: 0.57, Accuracy: 111/150 F1 (74.00%), F1 score: 79.35%\n","Model saved at step 50 with best accuracy 74.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.093691, loss_c: 0.122453, simclr_loss: 4.746790, grp_loss: 0.984080\n","Test set: Average loss: 0.46, Accuracy: 121/150 F1 (80.67%), F1 score: 76.94%\n","Model saved at step 60 with best accuracy 80.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.999466, loss_c: 0.054546, simclr_loss: 4.740238, grp_loss: 0.983968\n","Test set: Average loss: 0.42, Accuracy: 116/150 F1 (77.33%), F1 score: 90.23%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.009333, loss_c: 0.068441, simclr_loss: 4.738963, grp_loss: 0.985041\n","Test set: Average loss: 0.31, Accuracy: 123/150 F1 (82.00%), F1 score: 79.41%\n","Model saved at step 80 with best accuracy 82.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.965584, loss_c: 0.049016, simclr_loss: 4.732469, grp_loss: 0.986694\n","Test set: Average loss: 0.24, Accuracy: 131/150 F1 (87.33%), F1 score: 90.57%\n","Model saved at step 90 with best accuracy 87.33333333333333\n","Test set: Average loss: 0.23, Accuracy: 133/150 F1 (88.67%), F1 score: 86.22%\n","Model saved at step 99 with best accuracy 88.66666666666667\n","Test set: Average loss: 3.27, Accuracy: 52/150 F1 (34.67%), F1 score: 23.75%\n","Federated Best acc test 34.666667\n","Federated Best fscore test 23.753666\n","Ep: 0 lr: 0.01, loss_all: 23.867132, loss_c: 4.521582, simclr_loss: 4.836388, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.748875, loss_c: 0.205691, simclr_loss: 4.754916, grp_loss: 1.523517\n","Test set: Average loss: 0.57, Accuracy: 131/150 F1 (87.33%), F1 score: 95.43%\n","Model saved at step 10 with best accuracy 87.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.431833, loss_c: 0.038493, simclr_loss: 4.729120, grp_loss: 1.476861\n","Test set: Average loss: 0.64, Accuracy: 132/150 F1 (88.00%), F1 score: 77.54%\n","Model saved at step 20 with best accuracy 88.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.360355, loss_c: 0.016769, simclr_loss: 4.720816, grp_loss: 1.460324\n","Test set: Average loss: 0.23, Accuracy: 135/150 F1 (90.00%), F1 score: 86.00%\n","Model saved at step 30 with best accuracy 90.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.339476, loss_c: 0.002892, simclr_loss: 4.719996, grp_loss: 1.456598\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 96.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.330078, loss_c: 0.002322, simclr_loss: 4.717686, grp_loss: 1.457012\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.339291, loss_c: 0.002356, simclr_loss: 4.720322, grp_loss: 1.455649\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.35%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.319494, loss_c: 0.002664, simclr_loss: 4.715948, grp_loss: 1.453037\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.320869, loss_c: 0.001024, simclr_loss: 4.716335, grp_loss: 1.454504\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.316828, loss_c: 0.001821, simclr_loss: 4.715321, grp_loss: 1.453725\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 24.032511, loss_c: 4.687135, simclr_loss: 4.836344, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.750246, loss_c: 0.252524, simclr_loss: 4.749909, grp_loss: 1.498086\n","Test set: Average loss: 0.41, Accuracy: 126/150 F1 (84.00%), F1 score: 77.27%\n","Model saved at step 10 with best accuracy 84.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.515554, loss_c: 0.063798, simclr_loss: 4.742463, grp_loss: 1.481903\n","Test set: Average loss: 0.27, Accuracy: 136/150 F1 (90.67%), F1 score: 90.91%\n","Model saved at step 20 with best accuracy 90.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.452774, loss_c: 0.088555, simclr_loss: 4.724566, grp_loss: 1.465958\n","Test set: Average loss: 0.15, Accuracy: 140/150 F1 (93.33%), F1 score: 95.56%\n","Model saved at step 30 with best accuracy 93.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.393095, loss_c: 0.017925, simclr_loss: 4.728403, grp_loss: 1.461558\n","Test set: Average loss: 0.11, Accuracy: 140/150 F1 (93.33%), F1 score: 95.38%\n","Model saved at step 40 with best accuracy 93.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.341782, loss_c: 0.009060, simclr_loss: 4.717875, grp_loss: 1.461222\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.365202, loss_c: 0.008918, simclr_loss: 4.723877, grp_loss: 1.460777\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.51%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.350660, loss_c: 0.005485, simclr_loss: 4.722041, grp_loss: 1.457011\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.325655, loss_c: 0.001994, simclr_loss: 4.717092, grp_loss: 1.455294\n","Test set: Average loss: 0.07, Accuracy: 144/150 F1 (96.00%), F1 score: 95.50%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.321766, loss_c: 0.002609, simclr_loss: 4.716275, grp_loss: 1.454058\n","Test set: Average loss: 0.07, Accuracy: 144/150 F1 (96.00%), F1 score: 95.53%\n","Test set: Average loss: 0.08, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 24.861897, loss_c: 5.519448, simclr_loss: 4.835612, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.348753, loss_c: 0.312315, simclr_loss: 4.756675, grp_loss: 1.009738\n","Test set: Average loss: 0.55, Accuracy: 120/150 F1 (80.00%), F1 score: 82.05%\n","Model saved at step 10 with best accuracy 80.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.077486, loss_c: 0.102804, simclr_loss: 4.747161, grp_loss: 0.986037\n","Test set: Average loss: 0.43, Accuracy: 125/150 F1 (83.33%), F1 score: 72.68%\n","Model saved at step 20 with best accuracy 83.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.007765, loss_c: 0.053434, simclr_loss: 4.742891, grp_loss: 0.982766\n","Test set: Average loss: 0.28, Accuracy: 134/150 F1 (89.33%), F1 score: 95.37%\n","Model saved at step 30 with best accuracy 89.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.953562, loss_c: 0.048229, simclr_loss: 4.731410, grp_loss: 0.979694\n","Test set: Average loss: 0.24, Accuracy: 132/150 F1 (88.00%), F1 score: 95.40%\n","Ep: 50 lr: 0.005, loss_all: 19.950790, loss_c: 0.062242, simclr_loss: 4.728449, grp_loss: 0.974750\n","Test set: Average loss: 0.21, Accuracy: 139/150 F1 (92.67%), F1 score: 90.91%\n","Model saved at step 50 with best accuracy 92.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.895178, loss_c: 0.013950, simclr_loss: 4.726127, grp_loss: 0.976721\n","Test set: Average loss: 0.17, Accuracy: 139/150 F1 (92.67%), F1 score: 86.42%\n","Model saved at step 60 with best accuracy 92.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.879963, loss_c: 0.009337, simclr_loss: 4.723466, grp_loss: 0.976763\n","Test set: Average loss: 0.16, Accuracy: 139/150 F1 (92.67%), F1 score: 95.48%\n","Model saved at step 70 with best accuracy 92.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.946751, loss_c: 0.067203, simclr_loss: 4.726205, grp_loss: 0.974727\n","Test set: Average loss: 0.14, Accuracy: 139/150 F1 (92.67%), F1 score: 95.45%\n","Model saved at step 80 with best accuracy 92.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.990604, loss_c: 0.081255, simclr_loss: 4.732484, grp_loss: 0.979413\n","Test set: Average loss: 0.20, Accuracy: 140/150 F1 (93.33%), F1 score: 91.05%\n","Model saved at step 90 with best accuracy 93.33333333333333\n","Test set: Average loss: 0.11, Accuracy: 142/150 F1 (94.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 94.66666666666667\n","Test set: Average loss: 0.57, Accuracy: 124/150 F1 (82.67%), F1 score: 70.00%\n","Federated Best acc test 82.666667\n","Federated Best fscore test 70.000000\n","Ep: 0 lr: 0.01, loss_all: 19.223516, loss_c: 0.243163, simclr_loss: 4.745088, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.371374, loss_c: 0.024086, simclr_loss: 4.721632, grp_loss: 1.460760\n","Test set: Average loss: 0.44, Accuracy: 129/150 F1 (86.00%), F1 score: 77.62%\n","Model saved at step 10 with best accuracy 86.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.340090, loss_c: 0.015437, simclr_loss: 4.717094, grp_loss: 1.456275\n","Test set: Average loss: 0.09, Accuracy: 141/150 F1 (94.00%), F1 score: 95.43%\n","Model saved at step 20 with best accuracy 94.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.374804, loss_c: 0.029810, simclr_loss: 4.721628, grp_loss: 1.458481\n","Test set: Average loss: 0.16, Accuracy: 144/150 F1 (96.00%), F1 score: 91.02%\n","Model saved at step 30 with best accuracy 96.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.363132, loss_c: 0.029930, simclr_loss: 4.719569, grp_loss: 1.454926\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 90.91%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.324741, loss_c: 0.002909, simclr_loss: 4.716691, grp_loss: 1.455068\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 95.54%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.342180, loss_c: 0.001049, simclr_loss: 4.720989, grp_loss: 1.457175\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.325350, loss_c: 0.000846, simclr_loss: 4.717813, grp_loss: 1.453249\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318777, loss_c: 0.003958, simclr_loss: 4.715397, grp_loss: 1.453231\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.313082, loss_c: 0.000650, simclr_loss: 4.714721, grp_loss: 1.453548\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 19.189829, loss_c: 0.246610, simclr_loss: 4.735805, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.401972, loss_c: 0.019720, simclr_loss: 4.729284, grp_loss: 1.465114\n","Test set: Average loss: 0.45, Accuracy: 131/150 F1 (87.33%), F1 score: 82.46%\n","Model saved at step 10 with best accuracy 87.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.364286, loss_c: 0.022078, simclr_loss: 4.720216, grp_loss: 1.461345\n","Test set: Average loss: 0.50, Accuracy: 127/150 F1 (84.67%), F1 score: 71.37%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.346275, loss_c: 0.014260, simclr_loss: 4.718300, grp_loss: 1.458815\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 95.40%\n","Model saved at step 30 with best accuracy 96.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.333822, loss_c: 0.003603, simclr_loss: 4.718598, grp_loss: 1.455826\n","Test set: Average loss: 0.16, Accuracy: 143/150 F1 (95.33%), F1 score: 95.81%\n","Ep: 50 lr: 0.005, loss_all: 20.327339, loss_c: 0.003722, simclr_loss: 4.717385, grp_loss: 1.454077\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 96.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.380274, loss_c: 0.043610, simclr_loss: 4.718716, grp_loss: 1.461802\n","Test set: Average loss: 0.24, Accuracy: 134/150 F1 (89.33%), F1 score: 85.48%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.349527, loss_c: 0.004664, simclr_loss: 4.721102, grp_loss: 1.460456\n","Test set: Average loss: 0.13, Accuracy: 144/150 F1 (96.00%), F1 score: 95.38%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.366680, loss_c: 0.005815, simclr_loss: 4.723885, grp_loss: 1.465323\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.353012, loss_c: 0.002790, simclr_loss: 4.722435, grp_loss: 1.460484\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 95.41%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 95.31%\n","Ep: 0 lr: 0.01, loss_all: 19.574678, loss_c: 0.583704, simclr_loss: 4.747744, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.077553, loss_c: 0.115556, simclr_loss: 4.743101, grp_loss: 0.989596\n","Test set: Average loss: 0.52, Accuracy: 119/150 F1 (79.33%), F1 score: 81.94%\n","Model saved at step 10 with best accuracy 79.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.963720, loss_c: 0.046073, simclr_loss: 4.734898, grp_loss: 0.978056\n","Test set: Average loss: 0.57, Accuracy: 125/150 F1 (83.33%), F1 score: 71.66%\n","Model saved at step 20 with best accuracy 83.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.910921, loss_c: 0.019422, simclr_loss: 4.729389, grp_loss: 0.973942\n","Test set: Average loss: 0.33, Accuracy: 133/150 F1 (88.67%), F1 score: 81.55%\n","Model saved at step 30 with best accuracy 88.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.892033, loss_c: 0.013429, simclr_loss: 4.725477, grp_loss: 0.976694\n","Test set: Average loss: 0.24, Accuracy: 135/150 F1 (90.00%), F1 score: 85.51%\n","Model saved at step 40 with best accuracy 90.0\n","Ep: 50 lr: 0.005, loss_all: 19.881439, loss_c: 0.006757, simclr_loss: 4.725951, grp_loss: 0.970878\n","Test set: Average loss: 0.23, Accuracy: 138/150 F1 (92.00%), F1 score: 90.85%\n","Model saved at step 50 with best accuracy 92.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.948751, loss_c: 0.058285, simclr_loss: 4.729257, grp_loss: 0.973438\n","Test set: Average loss: 0.21, Accuracy: 138/150 F1 (92.00%), F1 score: 95.52%\n","Model saved at step 60 with best accuracy 92.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.917078, loss_c: 0.008529, simclr_loss: 4.733836, grp_loss: 0.973205\n","Test set: Average loss: 0.16, Accuracy: 137/150 F1 (91.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.975197, loss_c: 0.086796, simclr_loss: 4.727940, grp_loss: 0.976644\n","Test set: Average loss: 0.13, Accuracy: 140/150 F1 (93.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 93.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.008455, loss_c: 0.147049, simclr_loss: 4.720885, grp_loss: 0.977865\n","Test set: Average loss: 0.15, Accuracy: 140/150 F1 (93.33%), F1 score: 90.82%\n","Model saved at step 90 with best accuracy 93.33333333333333\n","Test set: Average loss: 0.12, Accuracy: 142/150 F1 (94.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 94.66666666666667\n","Test set: Average loss: 0.16, Accuracy: 139/150 F1 (92.67%), F1 score: 100.00%\n","Federated Best acc test 92.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.921265, loss_c: 0.036777, simclr_loss: 4.721122, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.345287, loss_c: 0.005180, simclr_loss: 4.720383, grp_loss: 1.458575\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.326565, loss_c: 0.000942, simclr_loss: 4.717459, grp_loss: 1.455788\n","Test set: Average loss: 0.05, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.328676, loss_c: 0.013272, simclr_loss: 4.715457, grp_loss: 1.453578\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.42%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.323112, loss_c: 0.002695, simclr_loss: 4.716727, grp_loss: 1.453510\n","Test set: Average loss: 0.07, Accuracy: 145/150 F1 (96.67%), F1 score: 95.43%\n","Ep: 50 lr: 0.005, loss_all: 20.316067, loss_c: 0.000576, simclr_loss: 4.715100, grp_loss: 1.455090\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.324480, loss_c: 0.001622, simclr_loss: 4.717268, grp_loss: 1.453789\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Model saved at step 60 with best accuracy 97.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.321024, loss_c: 0.000700, simclr_loss: 4.716425, grp_loss: 1.454624\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318317, loss_c: 0.001321, simclr_loss: 4.715877, grp_loss: 1.453491\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.339720, loss_c: 0.000536, simclr_loss: 4.720490, grp_loss: 1.457225\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 91.16%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.29%\n","Model saved at step 99 with best accuracy 97.33333333333333\n","Ep: 0 lr: 0.01, loss_all: 18.935041, loss_c: 0.047169, simclr_loss: 4.721968, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.397562, loss_c: 0.011684, simclr_loss: 4.732041, grp_loss: 1.457715\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.355736, loss_c: 0.023354, simclr_loss: 4.718820, grp_loss: 1.457102\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.330219, loss_c: 0.000797, simclr_loss: 4.718535, grp_loss: 1.455281\n","Test set: Average loss: 0.02, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.331347, loss_c: 0.001150, simclr_loss: 4.719182, grp_loss: 1.453466\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Ep: 50 lr: 0.005, loss_all: 20.319609, loss_c: 0.001599, simclr_loss: 4.716037, grp_loss: 1.453864\n","Test set: Average loss: 0.03, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.329638, loss_c: 0.002953, simclr_loss: 4.718236, grp_loss: 1.453741\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.320415, loss_c: 0.001426, simclr_loss: 4.715983, grp_loss: 1.455057\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.36%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.329931, loss_c: 0.002595, simclr_loss: 4.718350, grp_loss: 1.453936\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 95.39%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.328415, loss_c: 0.001444, simclr_loss: 4.718361, grp_loss: 1.453527\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 95.37%\n","Test set: Average loss: 0.13, Accuracy: 145/150 F1 (96.67%), F1 score: 91.32%\n","Ep: 0 lr: 0.01, loss_all: 19.179855, loss_c: 0.300481, simclr_loss: 4.719843, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.050169, loss_c: 0.159984, simclr_loss: 4.727861, grp_loss: 0.978741\n","Test set: Average loss: 0.50, Accuracy: 116/150 F1 (77.33%), F1 score: 77.27%\n","Model saved at step 10 with best accuracy 77.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.936241, loss_c: 0.046267, simclr_loss: 4.728754, grp_loss: 0.974959\n","Test set: Average loss: 0.32, Accuracy: 127/150 F1 (84.67%), F1 score: 83.25%\n","Model saved at step 20 with best accuracy 84.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.942989, loss_c: 0.047364, simclr_loss: 4.730203, grp_loss: 0.974816\n","Test set: Average loss: 0.23, Accuracy: 134/150 F1 (89.33%), F1 score: 90.91%\n","Model saved at step 30 with best accuracy 89.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.931883, loss_c: 0.042576, simclr_loss: 4.727423, grp_loss: 0.979617\n","Test set: Average loss: 0.25, Accuracy: 131/150 F1 (87.33%), F1 score: 95.31%\n","Ep: 50 lr: 0.005, loss_all: 19.983528, loss_c: 0.122935, simclr_loss: 4.721754, grp_loss: 0.973578\n","Test set: Average loss: 0.21, Accuracy: 134/150 F1 (89.33%), F1 score: 95.44%\n","Model saved at step 50 with best accuracy 89.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.916763, loss_c: 0.041783, simclr_loss: 4.723820, grp_loss: 0.979701\n","Test set: Average loss: 0.20, Accuracy: 137/150 F1 (91.33%), F1 score: 95.45%\n","Model saved at step 60 with best accuracy 91.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.013130, loss_c: 0.153131, simclr_loss: 4.720698, grp_loss: 0.977205\n","Test set: Average loss: 0.12, Accuracy: 142/150 F1 (94.67%), F1 score: 95.61%\n","Model saved at step 70 with best accuracy 94.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.949635, loss_c: 0.049893, simclr_loss: 4.730903, grp_loss: 0.976131\n","Test set: Average loss: 0.16, Accuracy: 140/150 F1 (93.33%), F1 score: 95.50%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.845181, loss_c: 0.003491, simclr_loss: 4.716905, grp_loss: 0.974072\n","Test set: Average loss: 0.13, Accuracy: 142/150 F1 (94.67%), F1 score: 95.32%\n","Model saved at step 90 with best accuracy 94.66666666666667\n","Test set: Average loss: 0.11, Accuracy: 141/150 F1 (94.00%), F1 score: 100.00%\n","Test set: Average loss: 0.15, Accuracy: 138/150 F1 (92.00%), F1 score: 100.00%\n","Federated Best acc test 92.000000\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.917585, loss_c: 0.026596, simclr_loss: 4.722747, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.332226, loss_c: 0.002574, simclr_loss: 4.718599, grp_loss: 1.455255\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 95.50%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.334204, loss_c: 0.003121, simclr_loss: 4.718962, grp_loss: 1.455234\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.321251, loss_c: 0.000884, simclr_loss: 4.716703, grp_loss: 1.453556\n","Test set: Average loss: 0.12, Accuracy: 145/150 F1 (96.67%), F1 score: 95.29%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.324421, loss_c: 0.001903, simclr_loss: 4.717312, grp_loss: 1.453270\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Ep: 50 lr: 0.005, loss_all: 20.327665, loss_c: 0.000742, simclr_loss: 4.718505, grp_loss: 1.452902\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.339355, loss_c: 0.002070, simclr_loss: 4.719965, grp_loss: 1.457425\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 95.35%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.327633, loss_c: 0.000307, simclr_loss: 4.718528, grp_loss: 1.453214\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.319786, loss_c: 0.003641, simclr_loss: 4.715620, grp_loss: 1.453664\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.51%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.314905, loss_c: 0.000175, simclr_loss: 4.715312, grp_loss: 1.453481\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 90.83%\n","Ep: 0 lr: 0.01, loss_all: 18.923071, loss_c: 0.027874, simclr_loss: 4.723799, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.354254, loss_c: 0.011878, simclr_loss: 4.721519, grp_loss: 1.456300\n","Test set: Average loss: 0.09, Accuracy: 144/150 F1 (96.00%), F1 score: 91.03%\n","Model saved at step 10 with best accuracy 96.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.334019, loss_c: 0.001146, simclr_loss: 4.719723, grp_loss: 1.453979\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.345409, loss_c: 0.025152, simclr_loss: 4.716493, grp_loss: 1.454287\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 95.48%\n","Model saved at step 30 with best accuracy 96.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.313025, loss_c: 0.000593, simclr_loss: 4.714845, grp_loss: 1.453051\n","Test set: Average loss: 0.23, Accuracy: 143/150 F1 (95.33%), F1 score: 86.08%\n","Ep: 50 lr: 0.005, loss_all: 20.329781, loss_c: 0.000889, simclr_loss: 4.717227, grp_loss: 1.459981\n","Test set: Average loss: 0.09, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.320375, loss_c: 0.002717, simclr_loss: 4.716090, grp_loss: 1.453298\n","Test set: Average loss: 0.07, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 96.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.336376, loss_c: 0.007426, simclr_loss: 4.718477, grp_loss: 1.455041\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 96.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.316570, loss_c: 0.001237, simclr_loss: 4.714631, grp_loss: 1.456812\n","Test set: Average loss: 0.12, Accuracy: 145/150 F1 (96.67%), F1 score: 89.77%\n","Model saved at step 80 with best accuracy 96.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.312366, loss_c: 0.000474, simclr_loss: 4.714709, grp_loss: 1.453056\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.41%\n","Model saved at step 90 with best accuracy 96.66666666666667\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.44%\n","Model saved at step 99 with best accuracy 96.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 18.988491, loss_c: 0.120311, simclr_loss: 4.717045, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.018871, loss_c: 0.120909, simclr_loss: 4.729455, grp_loss: 0.980143\n","Test set: Average loss: 0.81, Accuracy: 126/150 F1 (84.00%), F1 score: 55.29%\n","Model saved at step 10 with best accuracy 84.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.913895, loss_c: 0.022572, simclr_loss: 4.729338, grp_loss: 0.973972\n","Test set: Average loss: 0.83, Accuracy: 123/150 F1 (82.00%), F1 score: 66.43%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.883986, loss_c: 0.026389, simclr_loss: 4.720500, grp_loss: 0.975599\n","Test set: Average loss: 0.28, Accuracy: 132/150 F1 (88.00%), F1 score: 90.66%\n","Model saved at step 30 with best accuracy 88.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.877823, loss_c: 0.007945, simclr_loss: 4.724030, grp_loss: 0.973754\n","Test set: Average loss: 0.13, Accuracy: 141/150 F1 (94.00%), F1 score: 95.37%\n","Model saved at step 40 with best accuracy 94.0\n","Ep: 50 lr: 0.005, loss_all: 19.862141, loss_c: 0.013584, simclr_loss: 4.717524, grp_loss: 0.978461\n","Test set: Average loss: 0.16, Accuracy: 140/150 F1 (93.33%), F1 score: 95.45%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.893080, loss_c: 0.031786, simclr_loss: 4.721083, grp_loss: 0.976963\n","Test set: Average loss: 0.34, Accuracy: 137/150 F1 (91.33%), F1 score: 85.74%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.902054, loss_c: 0.042203, simclr_loss: 4.721820, grp_loss: 0.972571\n","Test set: Average loss: 0.31, Accuracy: 137/150 F1 (91.33%), F1 score: 95.45%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.896080, loss_c: 0.033013, simclr_loss: 4.721852, grp_loss: 0.975659\n","Test set: Average loss: 0.18, Accuracy: 142/150 F1 (94.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 94.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.855335, loss_c: 0.005353, simclr_loss: 4.719560, grp_loss: 0.971745\n","Test set: Average loss: 0.18, Accuracy: 143/150 F1 (95.33%), F1 score: 95.48%\n","Model saved at step 90 with best accuracy 95.33333333333333\n","Test set: Average loss: 0.10, Accuracy: 143/150 F1 (95.33%), F1 score: 95.59%\n","Model saved at step 99 with best accuracy 95.33333333333333\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Federated Best acc test 97.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.901373, loss_c: 0.010137, simclr_loss: 4.722809, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.329891, loss_c: 0.001755, simclr_loss: 4.717684, grp_loss: 1.457399\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.321001, loss_c: 0.002806, simclr_loss: 4.716177, grp_loss: 1.453490\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.326109, loss_c: 0.000925, simclr_loss: 4.717540, grp_loss: 1.455025\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.37%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.339378, loss_c: 0.008757, simclr_loss: 4.719226, grp_loss: 1.453717\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.318106, loss_c: 0.000287, simclr_loss: 4.715933, grp_loss: 1.454088\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.348143, loss_c: 0.000173, simclr_loss: 4.723243, grp_loss: 1.454998\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.316996, loss_c: 0.000082, simclr_loss: 4.715898, grp_loss: 1.453323\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.321476, loss_c: 0.000441, simclr_loss: 4.716913, grp_loss: 1.453383\n","Test set: Average loss: 0.09, Accuracy: 148/150 F1 (98.67%), F1 score: 95.38%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.337614, loss_c: 0.011080, simclr_loss: 4.717728, grp_loss: 1.455624\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.10, Accuracy: 148/150 F1 (98.67%), F1 score: 95.35%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 18.910812, loss_c: 0.020061, simclr_loss: 4.722688, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.322659, loss_c: 0.002416, simclr_loss: 4.716367, grp_loss: 1.454776\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.334608, loss_c: 0.016186, simclr_loss: 4.716088, grp_loss: 1.454069\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 90.97%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.348511, loss_c: 0.021131, simclr_loss: 4.718031, grp_loss: 1.455253\n","Test set: Average loss: 0.10, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.314911, loss_c: 0.000643, simclr_loss: 4.715270, grp_loss: 1.453187\n","Test set: Average loss: 0.05, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.333891, loss_c: 0.001909, simclr_loss: 4.719355, grp_loss: 1.454561\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.334299, loss_c: 0.008142, simclr_loss: 4.717515, grp_loss: 1.456096\n","Test set: Average loss: 0.05, Accuracy: 145/150 F1 (96.67%), F1 score: 95.36%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.350828, loss_c: 0.027184, simclr_loss: 4.716745, grp_loss: 1.456664\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 91.02%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.319952, loss_c: 0.002832, simclr_loss: 4.715948, grp_loss: 1.453328\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.335890, loss_c: 0.000745, simclr_loss: 4.720458, grp_loss: 1.453312\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.937895, loss_c: 0.067442, simclr_loss: 4.717613, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.952982, loss_c: 0.043654, simclr_loss: 4.731614, grp_loss: 0.982875\n","Test set: Average loss: 0.58, Accuracy: 132/150 F1 (88.00%), F1 score: 79.45%\n","Model saved at step 10 with best accuracy 88.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.954655, loss_c: 0.087888, simclr_loss: 4.723499, grp_loss: 0.972773\n","Test set: Average loss: 0.29, Accuracy: 136/150 F1 (90.67%), F1 score: 95.24%\n","Model saved at step 20 with best accuracy 90.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.903849, loss_c: 0.052429, simclr_loss: 4.719179, grp_loss: 0.974703\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.943344, loss_c: 0.065985, simclr_loss: 4.725555, grp_loss: 0.975138\n","Test set: Average loss: 0.43, Accuracy: 139/150 F1 (92.67%), F1 score: 84.98%\n","Ep: 50 lr: 0.005, loss_all: 20.000162, loss_c: 0.102358, simclr_loss: 4.731007, grp_loss: 0.973777\n","Test set: Average loss: 0.40, Accuracy: 132/150 F1 (88.00%), F1 score: 90.44%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.907608, loss_c: 0.013890, simclr_loss: 4.730594, grp_loss: 0.971344\n","Test set: Average loss: 0.16, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.973980, loss_c: 0.087028, simclr_loss: 4.727995, grp_loss: 0.974973\n","Test set: Average loss: 0.23, Accuracy: 140/150 F1 (93.33%), F1 score: 90.79%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.915272, loss_c: 0.060316, simclr_loss: 4.719647, grp_loss: 0.976368\n","Test set: Average loss: 0.15, Accuracy: 141/150 F1 (94.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.927395, loss_c: 0.018154, simclr_loss: 4.733434, grp_loss: 0.975504\n","Test set: Average loss: 0.21, Accuracy: 141/150 F1 (94.00%), F1 score: 90.97%\n","Test set: Average loss: 0.27, Accuracy: 140/150 F1 (93.33%), F1 score: 95.38%\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 95.45%\n","Federated Best acc test 96.000000\n","Federated Best fscore test 95.454545\n","Ep: 0 lr: 0.01, loss_all: 18.903366, loss_c: 0.001277, simclr_loss: 4.725522, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.327351, loss_c: 0.000581, simclr_loss: 4.718076, grp_loss: 1.454466\n","Test set: Average loss: 0.02, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.326258, loss_c: 0.002338, simclr_loss: 4.717099, grp_loss: 1.455525\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.324593, loss_c: 0.002747, simclr_loss: 4.716624, grp_loss: 1.455348\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 95.49%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.313927, loss_c: 0.000303, simclr_loss: 4.715198, grp_loss: 1.452832\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 99.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.334404, loss_c: 0.002149, simclr_loss: 4.719510, grp_loss: 1.454215\n","Test set: Average loss: 0.21, Accuracy: 143/150 F1 (95.33%), F1 score: 90.76%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.318966, loss_c: 0.001596, simclr_loss: 4.715982, grp_loss: 1.453442\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.317886, loss_c: 0.000204, simclr_loss: 4.716076, grp_loss: 1.453378\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.332571, loss_c: 0.001142, simclr_loss: 4.719426, grp_loss: 1.453724\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319588, loss_c: 0.000146, simclr_loss: 4.716565, grp_loss: 1.453181\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.38%\n","Ep: 0 lr: 0.01, loss_all: 18.921759, loss_c: 0.039419, simclr_loss: 4.720585, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.334702, loss_c: 0.002946, simclr_loss: 4.719519, grp_loss: 1.453680\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.327909, loss_c: 0.003508, simclr_loss: 4.716524, grp_loss: 1.458306\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.336901, loss_c: 0.004406, simclr_loss: 4.719231, grp_loss: 1.455572\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.341930, loss_c: 0.006414, simclr_loss: 4.720578, grp_loss: 1.453204\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.329754, loss_c: 0.000218, simclr_loss: 4.718634, grp_loss: 1.455002\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.324848, loss_c: 0.000440, simclr_loss: 4.717547, grp_loss: 1.454219\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.341625, loss_c: 0.000304, simclr_loss: 4.721727, grp_loss: 1.454410\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.335367, loss_c: 0.012609, simclr_loss: 4.717051, grp_loss: 1.454556\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319504, loss_c: 0.000160, simclr_loss: 4.716325, grp_loss: 1.454044\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 95.41%\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 19.152573, loss_c: 0.265322, simclr_loss: 4.721813, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.872486, loss_c: 0.024660, simclr_loss: 4.718422, grp_loss: 0.974137\n","Test set: Average loss: 0.50, Accuracy: 128/150 F1 (85.33%), F1 score: 71.58%\n","Model saved at step 10 with best accuracy 85.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.884880, loss_c: 0.013451, simclr_loss: 4.723388, grp_loss: 0.977879\n","Test set: Average loss: 0.57, Accuracy: 130/150 F1 (86.67%), F1 score: 70.60%\n","Model saved at step 20 with best accuracy 86.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.934393, loss_c: 0.058039, simclr_loss: 4.725539, grp_loss: 0.974197\n","Test set: Average loss: 0.14, Accuracy: 141/150 F1 (94.00%), F1 score: 95.38%\n","Model saved at step 30 with best accuracy 94.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.908274, loss_c: 0.052704, simclr_loss: 4.720544, grp_loss: 0.973394\n","Test set: Average loss: 0.12, Accuracy: 142/150 F1 (94.67%), F1 score: 95.40%\n","Model saved at step 40 with best accuracy 94.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 19.910288, loss_c: 0.061705, simclr_loss: 4.718901, grp_loss: 0.972981\n","Test set: Average loss: 0.04, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.921186, loss_c: 0.026097, simclr_loss: 4.728406, grp_loss: 0.981465\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.885859, loss_c: 0.031315, simclr_loss: 4.720849, grp_loss: 0.971148\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 95.32%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.839369, loss_c: 0.002442, simclr_loss: 4.715863, grp_loss: 0.973474\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 91.05%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.832783, loss_c: 0.001726, simclr_loss: 4.715096, grp_loss: 0.970673\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.04, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 95.53%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 95.530556\n","Ep: 0 lr: 0.01, loss_all: 18.885164, loss_c: 0.008136, simclr_loss: 4.719257, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.322140, loss_c: 0.001400, simclr_loss: 4.716930, grp_loss: 1.453021\n","Test set: Average loss: 0.07, Accuracy: 148/150 F1 (98.67%), F1 score: 95.53%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.334488, loss_c: 0.001576, simclr_loss: 4.719278, grp_loss: 1.455798\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.328955, loss_c: 0.001053, simclr_loss: 4.718693, grp_loss: 1.453131\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.316519, loss_c: 0.000390, simclr_loss: 4.715783, grp_loss: 1.452997\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 95.51%\n","Ep: 50 lr: 0.005, loss_all: 20.333097, loss_c: 0.001333, simclr_loss: 4.719661, grp_loss: 1.453119\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319965, loss_c: 0.001362, simclr_loss: 4.716145, grp_loss: 1.454024\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.328499, loss_c: 0.000186, simclr_loss: 4.718521, grp_loss: 1.454229\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.317299, loss_c: 0.000486, simclr_loss: 4.715829, grp_loss: 1.453497\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.315701, loss_c: 0.000234, simclr_loss: 4.715517, grp_loss: 1.453398\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 95.49%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 18.870476, loss_c: 0.004552, simclr_loss: 4.716481, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.370604, loss_c: 0.002022, simclr_loss: 4.726677, grp_loss: 1.461873\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.319771, loss_c: 0.002343, simclr_loss: 4.715964, grp_loss: 1.453572\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.358311, loss_c: 0.000321, simclr_loss: 4.724380, grp_loss: 1.460472\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.319893, loss_c: 0.000998, simclr_loss: 4.715570, grp_loss: 1.456613\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.324768, loss_c: 0.003161, simclr_loss: 4.716555, grp_loss: 1.455387\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319071, loss_c: 0.002054, simclr_loss: 4.716045, grp_loss: 1.452835\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.318935, loss_c: 0.001656, simclr_loss: 4.715403, grp_loss: 1.455670\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.325394, loss_c: 0.000929, simclr_loss: 4.716642, grp_loss: 1.457896\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 94.80%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.329512, loss_c: 0.001736, simclr_loss: 4.717933, grp_loss: 1.456046\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 95.49%\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Ep: 0 lr: 0.01, loss_all: 18.904926, loss_c: 0.024457, simclr_loss: 4.720118, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.855762, loss_c: 0.003108, simclr_loss: 4.719120, grp_loss: 0.976175\n","Test set: Average loss: 0.30, Accuracy: 137/150 F1 (91.33%), F1 score: 86.15%\n","Model saved at step 10 with best accuracy 91.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.897385, loss_c: 0.000730, simclr_loss: 4.729892, grp_loss: 0.977084\n","Test set: Average loss: 0.28, Accuracy: 136/150 F1 (90.67%), F1 score: 90.91%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.957312, loss_c: 0.069093, simclr_loss: 4.728056, grp_loss: 0.975995\n","Test set: Average loss: 0.21, Accuracy: 137/150 F1 (91.33%), F1 score: 81.82%\n","Model saved at step 30 with best accuracy 91.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.859776, loss_c: 0.003086, simclr_loss: 4.719948, grp_loss: 0.976896\n","Test set: Average loss: 0.31, Accuracy: 136/150 F1 (90.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.919830, loss_c: 0.043971, simclr_loss: 4.723683, grp_loss: 0.981126\n","Test set: Average loss: 0.16, Accuracy: 140/150 F1 (93.33%), F1 score: 95.37%\n","Model saved at step 50 with best accuracy 93.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.857714, loss_c: 0.006883, simclr_loss: 4.719406, grp_loss: 0.973206\n","Test set: Average loss: 0.08, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 96.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.997330, loss_c: 0.128557, simclr_loss: 4.721846, grp_loss: 0.981387\n","Test set: Average loss: 0.15, Accuracy: 141/150 F1 (94.00%), F1 score: 90.79%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.906921, loss_c: 0.003703, simclr_loss: 4.729947, grp_loss: 0.983433\n","Test set: Average loss: 0.11, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.885685, loss_c: 0.030848, simclr_loss: 4.719615, grp_loss: 0.976377\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.12, Accuracy: 144/150 F1 (96.00%), F1 score: 95.41%\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 95.42%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 95.422759\n","Ep: 0 lr: 0.01, loss_all: 18.871710, loss_c: 0.000165, simclr_loss: 4.717886, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.340902, loss_c: 0.000243, simclr_loss: 4.721196, grp_loss: 1.455876\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 90.91%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.316818, loss_c: 0.000225, simclr_loss: 4.715411, grp_loss: 1.454947\n","Test set: Average loss: 0.13, Accuracy: 148/150 F1 (98.67%), F1 score: 90.91%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.315815, loss_c: 0.000442, simclr_loss: 4.715577, grp_loss: 1.453065\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 95.51%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.333458, loss_c: 0.000059, simclr_loss: 4.719991, grp_loss: 1.453436\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.53%\n","Ep: 50 lr: 0.005, loss_all: 20.320585, loss_c: 0.000455, simclr_loss: 4.716550, grp_loss: 1.453930\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.320133, loss_c: 0.000134, simclr_loss: 4.716358, grp_loss: 1.454567\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.30%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.314913, loss_c: 0.000037, simclr_loss: 4.715521, grp_loss: 1.452789\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.326971, loss_c: 0.000092, simclr_loss: 4.717346, grp_loss: 1.457495\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.315529, loss_c: 0.003387, simclr_loss: 4.714801, grp_loss: 1.452937\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.924494, loss_c: 0.046906, simclr_loss: 4.719397, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.330538, loss_c: 0.000299, simclr_loss: 4.719088, grp_loss: 1.453888\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.344385, loss_c: 0.006620, simclr_loss: 4.720723, grp_loss: 1.454874\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.321278, loss_c: 0.000323, simclr_loss: 4.716982, grp_loss: 1.453028\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.50%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.320017, loss_c: 0.000270, simclr_loss: 4.716653, grp_loss: 1.453134\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.322372, loss_c: 0.002604, simclr_loss: 4.716361, grp_loss: 1.454325\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 86.36%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.322899, loss_c: 0.003113, simclr_loss: 4.716608, grp_loss: 1.453353\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 95.41%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.341265, loss_c: 0.001346, simclr_loss: 4.720955, grp_loss: 1.456096\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 95.47%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.329760, loss_c: 0.000130, simclr_loss: 4.719113, grp_loss: 1.453178\n","Test set: Average loss: 0.14, Accuracy: 145/150 F1 (96.67%), F1 score: 95.45%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.323586, loss_c: 0.000594, simclr_loss: 4.717242, grp_loss: 1.454023\n","Test set: Average loss: 0.14, Accuracy: 145/150 F1 (96.67%), F1 score: 95.53%\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.912230, loss_c: 0.043600, simclr_loss: 4.717157, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.857103, loss_c: 0.000719, simclr_loss: 4.721349, grp_loss: 0.970988\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.876513, loss_c: 0.029361, simclr_loss: 4.718587, grp_loss: 0.972804\n","Test set: Average loss: 0.25, Accuracy: 141/150 F1 (94.00%), F1 score: 86.51%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.901936, loss_c: 0.010275, simclr_loss: 4.728799, grp_loss: 0.976466\n","Test set: Average loss: 0.15, Accuracy: 142/150 F1 (94.67%), F1 score: 95.50%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.847050, loss_c: 0.003988, simclr_loss: 4.716996, grp_loss: 0.975079\n","Test set: Average loss: 0.21, Accuracy: 140/150 F1 (93.33%), F1 score: 90.91%\n","Ep: 50 lr: 0.005, loss_all: 19.858217, loss_c: 0.007999, simclr_loss: 4.718442, grp_loss: 0.976449\n","Test set: Average loss: 0.17, Accuracy: 138/150 F1 (92.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.870190, loss_c: 0.022514, simclr_loss: 4.719107, grp_loss: 0.971249\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.838604, loss_c: 0.000618, simclr_loss: 4.716856, grp_loss: 0.970562\n","Test set: Average loss: 0.10, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.844671, loss_c: 0.001083, simclr_loss: 4.718256, grp_loss: 0.970566\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.37%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.876204, loss_c: 0.038589, simclr_loss: 4.716631, grp_loss: 0.971090\n","Test set: Average loss: 0.07, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 90.27%\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 95.45%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 95.454545\n","Ep: 0 lr: 0.01, loss_all: 18.879553, loss_c: 0.003252, simclr_loss: 4.719075, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.312141, loss_c: 0.000186, simclr_loss: 4.714743, grp_loss: 1.452985\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.318336, loss_c: 0.001898, simclr_loss: 4.715179, grp_loss: 1.455720\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.327093, loss_c: 0.001069, simclr_loss: 4.716358, grp_loss: 1.460594\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.50%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.320770, loss_c: 0.005094, simclr_loss: 4.715512, grp_loss: 1.453627\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.424246, loss_c: 0.076037, simclr_loss: 4.722898, grp_loss: 1.456619\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.329609, loss_c: 0.000055, simclr_loss: 4.719079, grp_loss: 1.453236\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.310778, loss_c: 0.000488, simclr_loss: 4.714373, grp_loss: 1.452797\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.313892, loss_c: 0.000438, simclr_loss: 4.715144, grp_loss: 1.452878\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.334152, loss_c: 0.001883, simclr_loss: 4.719862, grp_loss: 1.452819\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.872055, loss_c: 0.002268, simclr_loss: 4.717447, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.335167, loss_c: 0.003323, simclr_loss: 4.719297, grp_loss: 1.454655\n","Test set: Average loss: 0.19, Accuracy: 144/150 F1 (96.00%), F1 score: 95.39%\n","Model saved at step 10 with best accuracy 96.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.341120, loss_c: 0.002274, simclr_loss: 4.721004, grp_loss: 1.454831\n","Test set: Average loss: 0.07, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.321125, loss_c: 0.003369, simclr_loss: 4.715638, grp_loss: 1.455203\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 95.45%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.318249, loss_c: 0.000293, simclr_loss: 4.716214, grp_loss: 1.453099\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 99.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.320768, loss_c: 0.000309, simclr_loss: 4.716152, grp_loss: 1.455850\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.347023, loss_c: 0.000335, simclr_loss: 4.723101, grp_loss: 1.454284\n","Test set: Average loss: 0.05, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 99.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.320772, loss_c: 0.000141, simclr_loss: 4.716671, grp_loss: 1.453945\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 99.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.400080, loss_c: 0.081510, simclr_loss: 4.716354, grp_loss: 1.453151\n","Test set: Average loss: 0.10, Accuracy: 149/150 F1 (99.33%), F1 score: 95.41%\n","Model saved at step 80 with best accuracy 99.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.317223, loss_c: 0.000495, simclr_loss: 4.715398, grp_loss: 1.455136\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 99.33333333333333\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 99.33333333333333\n","Ep: 0 lr: 0.01, loss_all: 18.927980, loss_c: 0.053911, simclr_loss: 4.718517, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.910826, loss_c: 0.033501, simclr_loss: 4.725414, grp_loss: 0.975667\n","Test set: Average loss: 0.08, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 95.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.873375, loss_c: 0.007214, simclr_loss: 4.722535, grp_loss: 0.976022\n","Test set: Average loss: 0.37, Accuracy: 131/150 F1 (87.33%), F1 score: 85.74%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.867287, loss_c: 0.006974, simclr_loss: 4.720817, grp_loss: 0.977047\n","Test set: Average loss: 0.53, Accuracy: 129/150 F1 (86.00%), F1 score: 78.40%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.918880, loss_c: 0.060464, simclr_loss: 4.721353, grp_loss: 0.973006\n","Test set: Average loss: 0.46, Accuracy: 132/150 F1 (88.00%), F1 score: 78.48%\n","Ep: 50 lr: 0.005, loss_all: 19.875168, loss_c: 0.043207, simclr_loss: 4.715431, grp_loss: 0.970236\n","Test set: Average loss: 0.16, Accuracy: 139/150 F1 (92.67%), F1 score: 95.49%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.882109, loss_c: 0.000545, simclr_loss: 4.726637, grp_loss: 0.975017\n","Test set: Average loss: 0.38, Accuracy: 127/150 F1 (84.67%), F1 score: 81.55%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.878637, loss_c: 0.035551, simclr_loss: 4.717947, grp_loss: 0.971300\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.36%\n","Model saved at step 70 with best accuracy 96.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.851305, loss_c: 0.000550, simclr_loss: 4.719587, grp_loss: 0.972405\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.41%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.856262, loss_c: 0.009128, simclr_loss: 4.718048, grp_loss: 0.974940\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.09, Accuracy: 144/150 F1 (96.00%), F1 score: 86.65%\n","Test set: Average loss: 0.05, Accuracy: 149/150 F1 (99.33%), F1 score: 95.42%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 95.422759\n","Ep: 0 lr: 0.01, loss_all: 18.869085, loss_c: 0.000487, simclr_loss: 4.717150, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.331947, loss_c: 0.002834, simclr_loss: 4.718884, grp_loss: 1.453579\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.324869, loss_c: 0.000231, simclr_loss: 4.717910, grp_loss: 1.452999\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 99.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.311241, loss_c: 0.000435, simclr_loss: 4.714469, grp_loss: 1.452930\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.38%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.337660, loss_c: 0.023905, simclr_loss: 4.714895, grp_loss: 1.454173\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 99.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.312347, loss_c: 0.000060, simclr_loss: 4.714754, grp_loss: 1.453270\n","Test set: Average loss: 0.06, Accuracy: 149/150 F1 (99.33%), F1 score: 95.41%\n","Model saved at step 50 with best accuracy 99.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.320099, loss_c: 0.000558, simclr_loss: 4.716349, grp_loss: 1.454146\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.361599, loss_c: 0.021650, simclr_loss: 4.720560, grp_loss: 1.457708\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318626, loss_c: 0.000458, simclr_loss: 4.715921, grp_loss: 1.454485\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.320854, loss_c: 0.000293, simclr_loss: 4.716923, grp_loss: 1.452870\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.54%\n","Ep: 0 lr: 0.01, loss_all: 18.878010, loss_c: 0.014240, simclr_loss: 4.715942, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.341459, loss_c: 0.004093, simclr_loss: 4.720499, grp_loss: 1.455370\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.318909, loss_c: 0.000847, simclr_loss: 4.715905, grp_loss: 1.454442\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.338928, loss_c: 0.009132, simclr_loss: 4.719248, grp_loss: 1.452804\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.31%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.327030, loss_c: 0.000363, simclr_loss: 4.717080, grp_loss: 1.458345\n","Test set: Average loss: 0.12, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.317154, loss_c: 0.000888, simclr_loss: 4.715852, grp_loss: 1.452859\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.340334, loss_c: 0.000706, simclr_loss: 4.721600, grp_loss: 1.453230\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.327797, loss_c: 0.000757, simclr_loss: 4.718477, grp_loss: 1.453133\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311195, loss_c: 0.000125, simclr_loss: 4.714534, grp_loss: 1.452936\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319117, loss_c: 0.000636, simclr_loss: 4.716422, grp_loss: 1.452793\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.50%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 18.938608, loss_c: 0.074141, simclr_loss: 4.716117, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.870489, loss_c: 0.002559, simclr_loss: 4.723288, grp_loss: 0.974777\n","Test set: Average loss: 0.22, Accuracy: 139/150 F1 (92.67%), F1 score: 86.43%\n","Model saved at step 10 with best accuracy 92.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.924948, loss_c: 0.062333, simclr_loss: 4.722311, grp_loss: 0.973371\n","Test set: Average loss: 0.15, Accuracy: 143/150 F1 (95.33%), F1 score: 90.57%\n","Model saved at step 20 with best accuracy 95.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.856544, loss_c: 0.016578, simclr_loss: 4.716587, grp_loss: 0.973618\n","Test set: Average loss: 0.52, Accuracy: 126/150 F1 (84.00%), F1 score: 90.61%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.870409, loss_c: 0.022244, simclr_loss: 4.718724, grp_loss: 0.973271\n","Test set: Average loss: 0.44, Accuracy: 135/150 F1 (90.00%), F1 score: 90.99%\n","Ep: 50 lr: 0.005, loss_all: 19.911402, loss_c: 0.030184, simclr_loss: 4.724919, grp_loss: 0.981541\n","Test set: Average loss: 0.16, Accuracy: 142/150 F1 (94.67%), F1 score: 90.76%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.891102, loss_c: 0.001190, simclr_loss: 4.727304, grp_loss: 0.980698\n","Test set: Average loss: 0.65, Accuracy: 126/150 F1 (84.00%), F1 score: 78.79%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.937469, loss_c: 0.085139, simclr_loss: 4.719315, grp_loss: 0.975074\n","Test set: Average loss: 0.28, Accuracy: 138/150 F1 (92.00%), F1 score: 89.85%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.891434, loss_c: 0.054166, simclr_loss: 4.716763, grp_loss: 0.970214\n","Test set: Average loss: 0.05, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 96.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.855610, loss_c: 0.006330, simclr_loss: 4.718517, grp_loss: 0.975210\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 95.38%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 99.33333333333333\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.866400, loss_c: 0.000026, simclr_loss: 4.716593, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.327679, loss_c: 0.017318, simclr_loss: 4.714364, grp_loss: 1.452904\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.328279, loss_c: 0.000026, simclr_loss: 4.718650, grp_loss: 1.453653\n","Test set: Average loss: 0.17, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.343491, loss_c: 0.009540, simclr_loss: 4.720281, grp_loss: 1.452828\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.312454, loss_c: 0.000033, simclr_loss: 4.714870, grp_loss: 1.452941\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 95.37%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.332203, loss_c: 0.000042, simclr_loss: 4.719843, grp_loss: 1.452789\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319580, loss_c: 0.000103, simclr_loss: 4.716416, grp_loss: 1.453811\n","Test set: Average loss: 0.25, Accuracy: 147/150 F1 (98.00%), F1 score: 91.10%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.323566, loss_c: 0.000977, simclr_loss: 4.716910, grp_loss: 1.454948\n","Test set: Average loss: 0.20, Accuracy: 147/150 F1 (98.00%), F1 score: 90.99%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311657, loss_c: 0.000056, simclr_loss: 4.714671, grp_loss: 1.452918\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 90.80%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.310719, loss_c: 0.000062, simclr_loss: 4.714468, grp_loss: 1.452787\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 18.866716, loss_c: 0.001827, simclr_loss: 4.716222, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.322395, loss_c: 0.001298, simclr_loss: 4.716506, grp_loss: 1.455072\n","Test set: Average loss: 0.41, Accuracy: 136/150 F1 (90.67%), F1 score: 90.85%\n","Model saved at step 10 with best accuracy 90.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.322420, loss_c: 0.000167, simclr_loss: 4.716989, grp_loss: 1.454299\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.337370, loss_c: 0.000602, simclr_loss: 4.720277, grp_loss: 1.455658\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 95.42%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.357252, loss_c: 0.000869, simclr_loss: 4.725635, grp_loss: 1.453843\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 95.51%\n","Ep: 50 lr: 0.005, loss_all: 20.343826, loss_c: 0.000203, simclr_loss: 4.722278, grp_loss: 1.454513\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.34%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.314798, loss_c: 0.000189, simclr_loss: 4.715368, grp_loss: 1.453137\n","Test set: Average loss: 0.07, Accuracy: 145/150 F1 (96.67%), F1 score: 95.35%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.333128, loss_c: 0.000244, simclr_loss: 4.719975, grp_loss: 1.452986\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311432, loss_c: 0.000194, simclr_loss: 4.714606, grp_loss: 1.452812\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.21%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.348631, loss_c: 0.021373, simclr_loss: 4.717861, grp_loss: 1.455815\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 95.37%\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.899021, loss_c: 0.030006, simclr_loss: 4.717254, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.856850, loss_c: 0.020818, simclr_loss: 4.716344, grp_loss: 0.970653\n","Test set: Average loss: 0.21, Accuracy: 145/150 F1 (96.67%), F1 score: 95.56%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.864750, loss_c: 0.002884, simclr_loss: 4.722592, grp_loss: 0.971499\n","Test set: Average loss: 0.18, Accuracy: 144/150 F1 (96.00%), F1 score: 95.40%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.031891, loss_c: 0.191197, simclr_loss: 4.716714, grp_loss: 0.973838\n","Test set: Average loss: 0.13, Accuracy: 144/150 F1 (96.00%), F1 score: 90.91%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.857183, loss_c: 0.005493, simclr_loss: 4.719485, grp_loss: 0.973749\n","Test set: Average loss: 0.40, Accuracy: 135/150 F1 (90.00%), F1 score: 90.91%\n","Ep: 50 lr: 0.005, loss_all: 19.937220, loss_c: 0.081122, simclr_loss: 4.721244, grp_loss: 0.971123\n","Test set: Average loss: 0.21, Accuracy: 139/150 F1 (92.67%), F1 score: 95.47%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.857866, loss_c: 0.008698, simclr_loss: 4.719477, grp_loss: 0.971262\n","Test set: Average loss: 0.24, Accuracy: 138/150 F1 (92.00%), F1 score: 90.85%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.845104, loss_c: 0.000631, simclr_loss: 4.718012, grp_loss: 0.972425\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 90.91%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.844677, loss_c: 0.002097, simclr_loss: 4.715453, grp_loss: 0.980769\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.841560, loss_c: 0.000539, simclr_loss: 4.716260, grp_loss: 0.975980\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 96.66666666666667\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.47%\n","Model saved at step 99 with best accuracy 96.66666666666667\n","Test set: Average loss: 0.07, Accuracy: 149/150 F1 (99.33%), F1 score: 95.23%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 95.228684\n","Ep: 0 lr: 0.01, loss_all: 18.869249, loss_c: 0.000120, simclr_loss: 4.717282, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.358967, loss_c: 0.002557, simclr_loss: 4.725748, grp_loss: 1.453418\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.321381, loss_c: 0.002887, simclr_loss: 4.716052, grp_loss: 1.454285\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.312428, loss_c: 0.000045, simclr_loss: 4.714851, grp_loss: 1.452976\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 99.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.318695, loss_c: 0.002092, simclr_loss: 4.715497, grp_loss: 1.454614\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 95.29%\n","Ep: 50 lr: 0.005, loss_all: 20.326866, loss_c: 0.000170, simclr_loss: 4.718392, grp_loss: 1.453127\n","Test set: Average loss: 0.14, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319548, loss_c: 0.001461, simclr_loss: 4.716268, grp_loss: 1.453017\n","Test set: Average loss: 0.21, Accuracy: 145/150 F1 (96.67%), F1 score: 95.38%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.312925, loss_c: 0.000034, simclr_loss: 4.715013, grp_loss: 1.452839\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.320303, loss_c: 0.000015, simclr_loss: 4.716585, grp_loss: 1.453949\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.320734, loss_c: 0.000018, simclr_loss: 4.716642, grp_loss: 1.454148\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Ep: 0 lr: 0.01, loss_all: 18.861361, loss_c: 0.001565, simclr_loss: 4.714949, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.318317, loss_c: 0.000481, simclr_loss: 4.716068, grp_loss: 1.453564\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.323389, loss_c: 0.006477, simclr_loss: 4.715899, grp_loss: 1.453315\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.314512, loss_c: 0.000125, simclr_loss: 4.715228, grp_loss: 1.453477\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.353863, loss_c: 0.022224, simclr_loss: 4.719301, grp_loss: 1.454434\n","Test set: Average loss: 0.25, Accuracy: 147/150 F1 (98.00%), F1 score: 85.76%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.321541, loss_c: 0.002955, simclr_loss: 4.716381, grp_loss: 1.453062\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.40%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319950, loss_c: 0.000611, simclr_loss: 4.716325, grp_loss: 1.454041\n","Test set: Average loss: 0.05, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 99.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.318859, loss_c: 0.000350, simclr_loss: 4.716416, grp_loss: 1.452845\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 99.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.341066, loss_c: 0.008201, simclr_loss: 4.718711, grp_loss: 1.458021\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 90.76%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.318295, loss_c: 0.000442, simclr_loss: 4.716023, grp_loss: 1.453761\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 99.33333333333333\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 99.33333333333333\n","Ep: 0 lr: 0.01, loss_all: 18.863810, loss_c: 0.002461, simclr_loss: 4.715337, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.850746, loss_c: 0.000347, simclr_loss: 4.719831, grp_loss: 0.971072\n","Test set: Average loss: 0.52, Accuracy: 129/150 F1 (86.00%), F1 score: 80.20%\n","Model saved at step 10 with best accuracy 86.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.860016, loss_c: 0.003292, simclr_loss: 4.720578, grp_loss: 0.974410\n","Test set: Average loss: 0.18, Accuracy: 141/150 F1 (94.00%), F1 score: 95.47%\n","Model saved at step 20 with best accuracy 94.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.854380, loss_c: 0.008470, simclr_loss: 4.717710, grp_loss: 0.975071\n","Test set: Average loss: 0.43, Accuracy: 131/150 F1 (87.33%), F1 score: 90.56%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.869152, loss_c: 0.013162, simclr_loss: 4.719249, grp_loss: 0.978995\n","Test set: Average loss: 0.16, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 95.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 19.881374, loss_c: 0.038707, simclr_loss: 4.718079, grp_loss: 0.970352\n","Test set: Average loss: 0.14, Accuracy: 142/150 F1 (94.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.873758, loss_c: 0.001307, simclr_loss: 4.724312, grp_loss: 0.975203\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 96.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.872435, loss_c: 0.001977, simclr_loss: 4.724248, grp_loss: 0.973464\n","Test set: Average loss: 0.09, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.843355, loss_c: 0.000408, simclr_loss: 4.718013, grp_loss: 0.970896\n","Test set: Average loss: 0.09, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.855051, loss_c: 0.000604, simclr_loss: 4.720048, grp_loss: 0.974254\n","Test set: Average loss: 0.09, Accuracy: 144/150 F1 (96.00%), F1 score: 95.41%\n","Test set: Average loss: 0.10, Accuracy: 144/150 F1 (96.00%), F1 score: 90.91%\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.867306, loss_c: 0.000123, simclr_loss: 4.716795, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.320763, loss_c: 0.004628, simclr_loss: 4.715572, grp_loss: 1.453848\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 95.42%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.313745, loss_c: 0.000051, simclr_loss: 4.715227, grp_loss: 1.452786\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.315800, loss_c: 0.000052, simclr_loss: 4.715737, grp_loss: 1.452799\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.310328, loss_c: 0.000055, simclr_loss: 4.714371, grp_loss: 1.452787\n","Test set: Average loss: 0.20, Accuracy: 147/150 F1 (98.00%), F1 score: 90.28%\n","Ep: 50 lr: 0.005, loss_all: 20.333347, loss_c: 0.000155, simclr_loss: 4.720015, grp_loss: 1.453134\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.322826, loss_c: 0.000025, simclr_loss: 4.717209, grp_loss: 1.453966\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.312485, loss_c: 0.000021, simclr_loss: 4.714905, grp_loss: 1.452845\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.322924, loss_c: 0.000012, simclr_loss: 4.716762, grp_loss: 1.455866\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.318588, loss_c: 0.000566, simclr_loss: 4.716312, grp_loss: 1.452774\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.915096, loss_c: 0.007936, simclr_loss: 4.726790, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.323586, loss_c: 0.002116, simclr_loss: 4.716699, grp_loss: 1.454675\n","Test set: Average loss: 0.18, Accuracy: 145/150 F1 (96.67%), F1 score: 90.67%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.336014, loss_c: 0.000289, simclr_loss: 4.720224, grp_loss: 1.454829\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.324476, loss_c: 0.000965, simclr_loss: 4.717291, grp_loss: 1.454345\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 95.42%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.324879, loss_c: 0.000108, simclr_loss: 4.717983, grp_loss: 1.452839\n","Test set: Average loss: 0.07, Accuracy: 148/150 F1 (98.67%), F1 score: 95.22%\n","Model saved at step 40 with best accuracy 98.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.314651, loss_c: 0.000325, simclr_loss: 4.715318, grp_loss: 1.453057\n","Test set: Average loss: 0.02, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.318834, loss_c: 0.004930, simclr_loss: 4.715253, grp_loss: 1.452893\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.329893, loss_c: 0.000348, simclr_loss: 4.718844, grp_loss: 1.454168\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 86.93%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.312696, loss_c: 0.000087, simclr_loss: 4.714941, grp_loss: 1.452844\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.321501, loss_c: 0.000068, simclr_loss: 4.717061, grp_loss: 1.453189\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 95.36%\n","Ep: 0 lr: 0.01, loss_all: 18.906618, loss_c: 0.027624, simclr_loss: 4.719748, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.859320, loss_c: 0.001280, simclr_loss: 4.720887, grp_loss: 0.974492\n","Test set: Average loss: 0.99, Accuracy: 121/150 F1 (80.67%), F1 score: 60.52%\n","Model saved at step 10 with best accuracy 80.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.910467, loss_c: 0.050936, simclr_loss: 4.719904, grp_loss: 0.979916\n","Test set: Average loss: 0.46, Accuracy: 130/150 F1 (86.67%), F1 score: 80.93%\n","Model saved at step 20 with best accuracy 86.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.868912, loss_c: 0.010113, simclr_loss: 4.721019, grp_loss: 0.974723\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.38%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.894461, loss_c: 0.025228, simclr_loss: 4.723595, grp_loss: 0.974854\n","Test set: Average loss: 0.20, Accuracy: 139/150 F1 (92.67%), F1 score: 86.56%\n","Ep: 50 lr: 0.005, loss_all: 19.875790, loss_c: 0.002425, simclr_loss: 4.724470, grp_loss: 0.975486\n","Test set: Average loss: 0.15, Accuracy: 141/150 F1 (94.00%), F1 score: 90.93%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.886623, loss_c: 0.042121, simclr_loss: 4.718451, grp_loss: 0.970697\n","Test set: Average loss: 0.02, Accuracy: 150/150 F1 (100.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 100.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.886148, loss_c: 0.053897, simclr_loss: 4.715430, grp_loss: 0.970533\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.844099, loss_c: 0.002515, simclr_loss: 4.717773, grp_loss: 0.970491\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.904242, loss_c: 0.029119, simclr_loss: 4.725267, grp_loss: 0.974055\n","Test set: Average loss: 0.07, Accuracy: 149/150 F1 (99.33%), F1 score: 95.54%\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.908318, loss_c: 0.041955, simclr_loss: 4.716591, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.320053, loss_c: 0.002022, simclr_loss: 4.716311, grp_loss: 1.452786\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.318726, loss_c: 0.000108, simclr_loss: 4.716215, grp_loss: 1.453757\n","Test set: Average loss: 0.18, Accuracy: 147/150 F1 (98.00%), F1 score: 95.32%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.328325, loss_c: 0.000088, simclr_loss: 4.718539, grp_loss: 1.454081\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.312872, loss_c: 0.000060, simclr_loss: 4.714964, grp_loss: 1.452958\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.327553, loss_c: 0.000017, simclr_loss: 4.718677, grp_loss: 1.452827\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.349052, loss_c: 0.000057, simclr_loss: 4.724033, grp_loss: 1.452863\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 95.26%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.311939, loss_c: 0.000027, simclr_loss: 4.714752, grp_loss: 1.452906\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.325115, loss_c: 0.006600, simclr_loss: 4.716312, grp_loss: 1.453265\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 99.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.340385, loss_c: 0.000373, simclr_loss: 4.721804, grp_loss: 1.452798\n","Test set: Average loss: 0.14, Accuracy: 148/150 F1 (98.67%), F1 score: 95.29%\n","Test set: Average loss: 0.15, Accuracy: 148/150 F1 (98.67%), F1 score: 95.42%\n","Ep: 0 lr: 0.01, loss_all: 18.872934, loss_c: 0.000506, simclr_loss: 4.718107, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.317537, loss_c: 0.005883, simclr_loss: 4.714616, grp_loss: 1.453189\n","Test set: Average loss: 0.27, Accuracy: 139/150 F1 (92.67%), F1 score: 90.51%\n","Model saved at step 10 with best accuracy 92.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.316454, loss_c: 0.000478, simclr_loss: 4.715703, grp_loss: 1.453164\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.317802, loss_c: 0.000289, simclr_loss: 4.715400, grp_loss: 1.455913\n","Test set: Average loss: 0.17, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.315914, loss_c: 0.000440, simclr_loss: 4.715464, grp_loss: 1.453619\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.326057, loss_c: 0.000025, simclr_loss: 4.718060, grp_loss: 1.453792\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 90.93%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.338760, loss_c: 0.002574, simclr_loss: 4.720810, grp_loss: 1.452945\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.322552, loss_c: 0.000295, simclr_loss: 4.716858, grp_loss: 1.454824\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.320757, loss_c: 0.003748, simclr_loss: 4.715543, grp_loss: 1.454838\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.327299, loss_c: 0.000030, simclr_loss: 4.718404, grp_loss: 1.453653\n","Test set: Average loss: 0.11, Accuracy: 148/150 F1 (98.67%), F1 score: 95.60%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.883863, loss_c: 0.004583, simclr_loss: 4.719820, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.890182, loss_c: 0.053315, simclr_loss: 4.715146, grp_loss: 0.976283\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.43%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.856459, loss_c: 0.001035, simclr_loss: 4.720984, grp_loss: 0.971486\n","Test set: Average loss: 0.11, Accuracy: 143/150 F1 (95.33%), F1 score: 95.43%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.863976, loss_c: 0.001462, simclr_loss: 4.717810, grp_loss: 0.991274\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.845051, loss_c: 0.002585, simclr_loss: 4.717649, grp_loss: 0.971867\n","Test set: Average loss: 0.07, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.852400, loss_c: 0.002020, simclr_loss: 4.718699, grp_loss: 0.975582\n","Test set: Average loss: 0.04, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.900055, loss_c: 0.062475, simclr_loss: 4.716912, grp_loss: 0.969932\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.961550, loss_c: 0.094444, simclr_loss: 4.723800, grp_loss: 0.971907\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 99.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.847279, loss_c: 0.005320, simclr_loss: 4.717647, grp_loss: 0.971371\n","Test set: Average loss: 0.05, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 99.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.846083, loss_c: 0.001147, simclr_loss: 4.718314, grp_loss: 0.971681\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Federated Best acc test 98.000000\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.863005, loss_c: 0.000098, simclr_loss: 4.715726, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.315407, loss_c: 0.001134, simclr_loss: 4.715342, grp_loss: 1.452906\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.345222, loss_c: 0.031662, simclr_loss: 4.715025, grp_loss: 1.453460\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.326057, loss_c: 0.000954, simclr_loss: 4.717485, grp_loss: 1.455162\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.311590, loss_c: 0.000004, simclr_loss: 4.714656, grp_loss: 1.452963\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.328165, loss_c: 0.000038, simclr_loss: 4.718777, grp_loss: 1.453019\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 95.49%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.312729, loss_c: 0.000078, simclr_loss: 4.714889, grp_loss: 1.453097\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.314436, loss_c: 0.000372, simclr_loss: 4.715323, grp_loss: 1.452771\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.317184, loss_c: 0.000262, simclr_loss: 4.716017, grp_loss: 1.452855\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.312843, loss_c: 0.000141, simclr_loss: 4.714953, grp_loss: 1.452888\n","Test set: Average loss: 0.34, Accuracy: 147/150 F1 (98.00%), F1 score: 90.61%\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.879730, loss_c: 0.007907, simclr_loss: 4.717956, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.359249, loss_c: 0.028670, simclr_loss: 4.717718, grp_loss: 1.459707\n","Test set: Average loss: 0.35, Accuracy: 145/150 F1 (96.67%), F1 score: 85.48%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.321621, loss_c: 0.000269, simclr_loss: 4.715734, grp_loss: 1.458417\n","Test set: Average loss: 0.24, Accuracy: 144/150 F1 (96.00%), F1 score: 95.45%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.316229, loss_c: 0.000560, simclr_loss: 4.715566, grp_loss: 1.453405\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.40%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.313286, loss_c: 0.000181, simclr_loss: 4.715047, grp_loss: 1.452917\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 95.45%\n","Ep: 50 lr: 0.005, loss_all: 20.336342, loss_c: 0.007893, simclr_loss: 4.718242, grp_loss: 1.455482\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 99.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.324764, loss_c: 0.001047, simclr_loss: 4.717548, grp_loss: 1.453525\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.329552, loss_c: 0.000068, simclr_loss: 4.718925, grp_loss: 1.453785\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.48%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.313293, loss_c: 0.000277, simclr_loss: 4.715038, grp_loss: 1.452864\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.344917, loss_c: 0.001996, simclr_loss: 4.722330, grp_loss: 1.453602\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.906118, loss_c: 0.037789, simclr_loss: 4.717083, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.837822, loss_c: 0.002324, simclr_loss: 4.715429, grp_loss: 0.973783\n","Test set: Average loss: 0.14, Accuracy: 142/150 F1 (94.67%), F1 score: 95.41%\n","Model saved at step 10 with best accuracy 94.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.840174, loss_c: 0.000932, simclr_loss: 4.717177, grp_loss: 0.970534\n","Test set: Average loss: 0.07, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 95.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.846737, loss_c: 0.001508, simclr_loss: 4.716330, grp_loss: 0.979908\n","Test set: Average loss: 0.24, Accuracy: 137/150 F1 (91.33%), F1 score: 95.35%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.852058, loss_c: 0.003076, simclr_loss: 4.718257, grp_loss: 0.975954\n","Test set: Average loss: 0.05, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 19.848646, loss_c: 0.000172, simclr_loss: 4.718332, grp_loss: 0.975147\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.840748, loss_c: 0.000865, simclr_loss: 4.716216, grp_loss: 0.975017\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.851553, loss_c: 0.000191, simclr_loss: 4.720236, grp_loss: 0.970418\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 95.39%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.845480, loss_c: 0.000766, simclr_loss: 4.718241, grp_loss: 0.971751\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 90.66%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.831385, loss_c: 0.000356, simclr_loss: 4.715254, grp_loss: 0.970012\n","Test set: Average loss: 0.05, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.869171, loss_c: 0.000012, simclr_loss: 4.717290, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.317436, loss_c: 0.000031, simclr_loss: 4.715525, grp_loss: 1.455304\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.315115, loss_c: 0.000083, simclr_loss: 4.715553, grp_loss: 1.452820\n","Test set: Average loss: 0.18, Accuracy: 147/150 F1 (98.00%), F1 score: 95.38%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.324905, loss_c: 0.001043, simclr_loss: 4.717274, grp_loss: 1.454767\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 99.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.314226, loss_c: 0.000111, simclr_loss: 4.715333, grp_loss: 1.452785\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 95.41%\n","Ep: 50 lr: 0.005, loss_all: 20.319176, loss_c: 0.000170, simclr_loss: 4.716276, grp_loss: 1.453901\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.326017, loss_c: 0.000048, simclr_loss: 4.718295, grp_loss: 1.452789\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.327646, loss_c: 0.000037, simclr_loss: 4.718701, grp_loss: 1.452804\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311661, loss_c: 0.000019, simclr_loss: 4.714611, grp_loss: 1.453197\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.335781, loss_c: 0.010548, simclr_loss: 4.717733, grp_loss: 1.454300\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 95.41%\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.861599, loss_c: 0.000114, simclr_loss: 4.715371, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.316381, loss_c: 0.000027, simclr_loss: 4.715498, grp_loss: 1.454363\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.320839, loss_c: 0.000074, simclr_loss: 4.716717, grp_loss: 1.453898\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.328304, loss_c: 0.000439, simclr_loss: 4.718110, grp_loss: 1.455424\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.322838, loss_c: 0.000026, simclr_loss: 4.717499, grp_loss: 1.452817\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.312918, loss_c: 0.000010, simclr_loss: 4.714950, grp_loss: 1.453108\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 95.45%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.328323, loss_c: 0.001492, simclr_loss: 4.718219, grp_loss: 1.453956\n","Test set: Average loss: 0.21, Accuracy: 144/150 F1 (96.00%), F1 score: 95.38%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.335262, loss_c: 0.000106, simclr_loss: 4.720239, grp_loss: 1.454202\n","Test set: Average loss: 0.15, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.319630, loss_c: 0.000114, simclr_loss: 4.716661, grp_loss: 1.452870\n","Test set: Average loss: 0.12, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.348169, loss_c: 0.001007, simclr_loss: 4.723523, grp_loss: 1.453069\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.875225, loss_c: 0.010911, simclr_loss: 4.716079, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.830418, loss_c: 0.000161, simclr_loss: 4.714860, grp_loss: 0.970818\n","Test set: Average loss: 0.07, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 95.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.851450, loss_c: 0.005941, simclr_loss: 4.718600, grp_loss: 0.971108\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 99.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.854317, loss_c: 0.001864, simclr_loss: 4.720409, grp_loss: 0.970816\n","Test set: Average loss: 0.12, Accuracy: 139/150 F1 (92.67%), F1 score: 90.79%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.949873, loss_c: 0.108095, simclr_loss: 4.716629, grp_loss: 0.975262\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.852308, loss_c: 0.001552, simclr_loss: 4.719500, grp_loss: 0.972755\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.908752, loss_c: 0.044125, simclr_loss: 4.721426, grp_loss: 0.978923\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.872982, loss_c: 0.011366, simclr_loss: 4.719917, grp_loss: 0.981950\n","Test set: Average loss: 0.15, Accuracy: 144/150 F1 (96.00%), F1 score: 90.41%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.847692, loss_c: 0.000119, simclr_loss: 4.717499, grp_loss: 0.977579\n","Test set: Average loss: 0.10, Accuracy: 143/150 F1 (95.33%), F1 score: 95.38%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.860153, loss_c: 0.009033, simclr_loss: 4.718818, grp_loss: 0.975848\n","Test set: Average loss: 0.07, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Test set: Average loss: 0.07, Accuracy: 144/150 F1 (96.00%), F1 score: 95.45%\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.865761, loss_c: 0.002865, simclr_loss: 4.715724, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.318480, loss_c: 0.000011, simclr_loss: 4.716416, grp_loss: 1.452802\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.42%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.329895, loss_c: 0.000251, simclr_loss: 4.718370, grp_loss: 1.456161\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 95.45%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.314436, loss_c: 0.000038, simclr_loss: 4.715376, grp_loss: 1.452894\n","Test set: Average loss: 0.23, Accuracy: 144/150 F1 (96.00%), F1 score: 95.53%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.323420, loss_c: 0.000361, simclr_loss: 4.717555, grp_loss: 1.452841\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.327528, loss_c: 0.002504, simclr_loss: 4.717997, grp_loss: 1.453037\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.39%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.317228, loss_c: 0.000109, simclr_loss: 4.715992, grp_loss: 1.453152\n","Test set: Average loss: 0.05, Accuracy: 149/150 F1 (99.33%), F1 score: 95.47%\n","Model saved at step 60 with best accuracy 99.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.342911, loss_c: 0.005887, simclr_loss: 4.720695, grp_loss: 1.454243\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.327328, loss_c: 0.000032, simclr_loss: 4.718631, grp_loss: 1.452771\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.36%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.310946, loss_c: 0.000010, simclr_loss: 4.714540, grp_loss: 1.452773\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Ep: 0 lr: 0.01, loss_all: 18.861086, loss_c: 0.002931, simclr_loss: 4.714539, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.334761, loss_c: 0.007260, simclr_loss: 4.718664, grp_loss: 1.452845\n","Test set: Average loss: 0.34, Accuracy: 138/150 F1 (92.00%), F1 score: 90.96%\n","Model saved at step 10 with best accuracy 92.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.312855, loss_c: 0.001161, simclr_loss: 4.714689, grp_loss: 1.452938\n","Test set: Average loss: 0.21, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.322819, loss_c: 0.000033, simclr_loss: 4.717442, grp_loss: 1.453018\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.315281, loss_c: 0.000159, simclr_loss: 4.715550, grp_loss: 1.452921\n","Test set: Average loss: 0.13, Accuracy: 143/150 F1 (95.33%), F1 score: 95.29%\n","Ep: 50 lr: 0.005, loss_all: 20.314110, loss_c: 0.000013, simclr_loss: 4.715233, grp_loss: 1.453164\n","Test set: Average loss: 0.19, Accuracy: 140/150 F1 (93.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319252, loss_c: 0.000040, simclr_loss: 4.716562, grp_loss: 1.452963\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 95.43%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.312868, loss_c: 0.000040, simclr_loss: 4.714950, grp_loss: 1.453030\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.48%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.312185, loss_c: 0.000026, simclr_loss: 4.714767, grp_loss: 1.453093\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.327442, loss_c: 0.000106, simclr_loss: 4.717888, grp_loss: 1.455784\n","Test set: Average loss: 0.17, Accuracy: 146/150 F1 (97.33%), F1 score: 95.52%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 95.44%\n","Ep: 0 lr: 0.01, loss_all: 18.870764, loss_c: 0.001532, simclr_loss: 4.717308, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.834305, loss_c: 0.000973, simclr_loss: 4.715842, grp_loss: 0.969962\n","Test set: Average loss: 0.04, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.835779, loss_c: 0.003626, simclr_loss: 4.714560, grp_loss: 0.973915\n","Test set: Average loss: 0.04, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.840960, loss_c: 0.001553, simclr_loss: 4.717114, grp_loss: 0.970950\n","Test set: Average loss: 0.23, Accuracy: 140/150 F1 (93.33%), F1 score: 91.35%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.901936, loss_c: 0.059788, simclr_loss: 4.717896, grp_loss: 0.970566\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 95.38%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 19.863924, loss_c: 0.003315, simclr_loss: 4.720459, grp_loss: 0.978773\n","Test set: Average loss: 0.05, Accuracy: 146/150 F1 (97.33%), F1 score: 95.32%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.895761, loss_c: 0.000223, simclr_loss: 4.730109, grp_loss: 0.975101\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 95.52%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.856560, loss_c: 0.000973, simclr_loss: 4.719882, grp_loss: 0.976062\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 95.42%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.836285, loss_c: 0.000467, simclr_loss: 4.716444, grp_loss: 0.970044\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.849010, loss_c: 0.006263, simclr_loss: 4.717380, grp_loss: 0.973228\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.873964, loss_c: 0.000027, simclr_loss: 4.718484, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.322245, loss_c: 0.000050, simclr_loss: 4.717335, grp_loss: 1.452855\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.316891, loss_c: 0.000101, simclr_loss: 4.715990, grp_loss: 1.452830\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.43%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.311399, loss_c: 0.000081, simclr_loss: 4.714615, grp_loss: 1.452858\n","Test set: Average loss: 0.28, Accuracy: 146/150 F1 (97.33%), F1 score: 91.10%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.346024, loss_c: 0.000798, simclr_loss: 4.722991, grp_loss: 1.453261\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.316465, loss_c: 0.000010, simclr_loss: 4.715919, grp_loss: 1.452781\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 95.26%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.321749, loss_c: 0.000208, simclr_loss: 4.716986, grp_loss: 1.453598\n","Test set: Average loss: 0.24, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.327511, loss_c: 0.000021, simclr_loss: 4.718676, grp_loss: 1.452785\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.329065, loss_c: 0.000150, simclr_loss: 4.718678, grp_loss: 1.454202\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.318344, loss_c: 0.000233, simclr_loss: 4.716275, grp_loss: 1.453010\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 90.72%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 18.868280, loss_c: 0.000508, simclr_loss: 4.716943, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.327377, loss_c: 0.000042, simclr_loss: 4.718451, grp_loss: 1.453530\n","Test set: Average loss: 0.21, Accuracy: 144/150 F1 (96.00%), F1 score: 95.42%\n","Model saved at step 10 with best accuracy 96.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.314529, loss_c: 0.000145, simclr_loss: 4.715252, grp_loss: 1.453376\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.317671, loss_c: 0.000465, simclr_loss: 4.715804, grp_loss: 1.453989\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.331112, loss_c: 0.000049, simclr_loss: 4.719448, grp_loss: 1.453273\n","Test set: Average loss: 0.28, Accuracy: 146/150 F1 (97.33%), F1 score: 95.35%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.331734, loss_c: 0.000040, simclr_loss: 4.719707, grp_loss: 1.452868\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.318085, loss_c: 0.000041, simclr_loss: 4.716309, grp_loss: 1.452806\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.317022, loss_c: 0.000008, simclr_loss: 4.715804, grp_loss: 1.453798\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318066, loss_c: 0.000033, simclr_loss: 4.716187, grp_loss: 1.453285\n","Test set: Average loss: 0.07, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319386, loss_c: 0.000215, simclr_loss: 4.716509, grp_loss: 1.453135\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Ep: 0 lr: 0.01, loss_all: 18.860655, loss_c: 0.000077, simclr_loss: 4.715145, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.884964, loss_c: 0.035561, simclr_loss: 4.719829, grp_loss: 0.970089\n","Test set: Average loss: 0.31, Accuracy: 136/150 F1 (90.67%), F1 score: 90.85%\n","Model saved at step 10 with best accuracy 90.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.842615, loss_c: 0.001695, simclr_loss: 4.717436, grp_loss: 0.971177\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 95.56%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.847887, loss_c: 0.011380, simclr_loss: 4.716515, grp_loss: 0.970447\n","Test set: Average loss: 0.12, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.858629, loss_c: 0.012183, simclr_loss: 4.718763, grp_loss: 0.971396\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.846006, loss_c: 0.011201, simclr_loss: 4.716155, grp_loss: 0.970186\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.855417, loss_c: 0.005760, simclr_loss: 4.719083, grp_loss: 0.973324\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.40%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.844357, loss_c: 0.000496, simclr_loss: 4.718102, grp_loss: 0.971451\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.866253, loss_c: 0.030296, simclr_loss: 4.716484, grp_loss: 0.970020\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.856544, loss_c: 0.015419, simclr_loss: 4.717781, grp_loss: 0.970002\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 91.16%\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Federated Best acc test 98.000000\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.864786, loss_c: 0.000049, simclr_loss: 4.716184, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.321529, loss_c: 0.000085, simclr_loss: 4.717120, grp_loss: 1.452963\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.312214, loss_c: 0.000058, simclr_loss: 4.714817, grp_loss: 1.452888\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.311733, loss_c: 0.000013, simclr_loss: 4.714736, grp_loss: 1.452775\n","Test set: Average loss: 0.22, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.339022, loss_c: 0.000057, simclr_loss: 4.721510, grp_loss: 1.452924\n","Test set: Average loss: 0.32, Accuracy: 146/150 F1 (97.33%), F1 score: 95.32%\n","Ep: 50 lr: 0.005, loss_all: 20.329334, loss_c: 0.000180, simclr_loss: 4.718872, grp_loss: 1.453664\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 95.60%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.319811, loss_c: 0.000074, simclr_loss: 4.716345, grp_loss: 1.454357\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.330399, loss_c: 0.000016, simclr_loss: 4.719035, grp_loss: 1.454242\n","Test set: Average loss: 0.14, Accuracy: 148/150 F1 (98.67%), F1 score: 95.43%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.322519, loss_c: 0.000064, simclr_loss: 4.717319, grp_loss: 1.453177\n","Test set: Average loss: 0.08, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.66666666666667\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.313118, loss_c: 0.000008, simclr_loss: 4.715004, grp_loss: 1.453095\n","Test set: Average loss: 0.08, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.08, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 18.863405, loss_c: 0.000240, simclr_loss: 4.715791, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.321003, loss_c: 0.000212, simclr_loss: 4.716779, grp_loss: 1.453674\n","Test set: Average loss: 0.17, Accuracy: 143/150 F1 (95.33%), F1 score: 91.23%\n","Model saved at step 10 with best accuracy 95.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.319906, loss_c: 0.000123, simclr_loss: 4.716569, grp_loss: 1.453509\n","Test set: Average loss: 0.15, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 95.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.318821, loss_c: 0.000182, simclr_loss: 4.716427, grp_loss: 1.452929\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 91.05%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.312210, loss_c: 0.000018, simclr_loss: 4.714814, grp_loss: 1.452936\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.330900, loss_c: 0.008848, simclr_loss: 4.717317, grp_loss: 1.452786\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.40%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.331438, loss_c: 0.000026, simclr_loss: 4.719343, grp_loss: 1.454041\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.329569, loss_c: 0.000527, simclr_loss: 4.718780, grp_loss: 1.453925\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.360270, loss_c: 0.019029, simclr_loss: 4.721766, grp_loss: 1.454175\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.315060, loss_c: 0.000100, simclr_loss: 4.715534, grp_loss: 1.452823\n","Test set: Average loss: 0.12, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.13, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.885195, loss_c: 0.009394, simclr_loss: 4.718950, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.840954, loss_c: 0.001614, simclr_loss: 4.716798, grp_loss: 0.972150\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 90.85%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.881426, loss_c: 0.021334, simclr_loss: 4.722439, grp_loss: 0.970337\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.847746, loss_c: 0.000179, simclr_loss: 4.717309, grp_loss: 0.978330\n","Test set: Average loss: 0.13, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.855556, loss_c: 0.007336, simclr_loss: 4.718979, grp_loss: 0.972306\n","Test set: Average loss: 0.27, Accuracy: 138/150 F1 (92.00%), F1 score: 90.99%\n","Ep: 50 lr: 0.005, loss_all: 19.831751, loss_c: 0.000036, simclr_loss: 4.715410, grp_loss: 0.970073\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.853134, loss_c: 0.000209, simclr_loss: 4.719532, grp_loss: 0.974797\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.860085, loss_c: 0.000129, simclr_loss: 4.720434, grp_loss: 0.978219\n","Test set: Average loss: 0.22, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.872843, loss_c: 0.021243, simclr_loss: 4.719913, grp_loss: 0.971946\n","Test set: Average loss: 0.18, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.875273, loss_c: 0.016848, simclr_loss: 4.721923, grp_loss: 0.970734\n","Test set: Average loss: 0.19, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.32, Accuracy: 146/150 F1 (97.33%), F1 score: 91.67%\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.860924, loss_c: 0.000399, simclr_loss: 4.715131, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.335257, loss_c: 0.000021, simclr_loss: 4.720551, grp_loss: 1.453029\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.329323, loss_c: 0.014399, simclr_loss: 4.715520, grp_loss: 1.452844\n","Test set: Average loss: 0.15, Accuracy: 144/150 F1 (96.00%), F1 score: 95.37%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.318787, loss_c: 0.000097, simclr_loss: 4.716393, grp_loss: 1.453116\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.321871, loss_c: 0.001230, simclr_loss: 4.716619, grp_loss: 1.454163\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.50%\n","Ep: 50 lr: 0.005, loss_all: 20.314171, loss_c: 0.001703, simclr_loss: 4.714880, grp_loss: 1.452950\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.323719, loss_c: 0.009112, simclr_loss: 4.715317, grp_loss: 1.453341\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.316626, loss_c: 0.000021, simclr_loss: 4.715950, grp_loss: 1.452804\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.325449, loss_c: 0.000015, simclr_loss: 4.717822, grp_loss: 1.454145\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319523, loss_c: 0.000184, simclr_loss: 4.716622, grp_loss: 1.452851\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.858234, loss_c: 0.000024, simclr_loss: 4.714552, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.311844, loss_c: 0.000075, simclr_loss: 4.714748, grp_loss: 1.452776\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.325722, loss_c: 0.007441, simclr_loss: 4.716307, grp_loss: 1.453053\n","Test set: Average loss: 0.27, Accuracy: 142/150 F1 (94.67%), F1 score: 91.25%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.314634, loss_c: 0.000244, simclr_loss: 4.715393, grp_loss: 1.452819\n","Test set: Average loss: 0.54, Accuracy: 136/150 F1 (90.67%), F1 score: 87.06%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.313347, loss_c: 0.000008, simclr_loss: 4.715065, grp_loss: 1.453079\n","Test set: Average loss: 0.20, Accuracy: 142/150 F1 (94.67%), F1 score: 95.51%\n","Ep: 50 lr: 0.005, loss_all: 20.312693, loss_c: 0.000072, simclr_loss: 4.714950, grp_loss: 1.452821\n","Test set: Average loss: 0.25, Accuracy: 147/150 F1 (98.00%), F1 score: 90.61%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.313549, loss_c: 0.000006, simclr_loss: 4.715185, grp_loss: 1.452805\n","Test set: Average loss: 0.64, Accuracy: 138/150 F1 (92.00%), F1 score: 77.48%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.318378, loss_c: 0.000022, simclr_loss: 4.716312, grp_loss: 1.453107\n","Test set: Average loss: 0.29, Accuracy: 138/150 F1 (92.00%), F1 score: 91.06%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.342215, loss_c: 0.001103, simclr_loss: 4.721395, grp_loss: 1.455531\n","Test set: Average loss: 0.21, Accuracy: 143/150 F1 (95.33%), F1 score: 95.39%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.321012, loss_c: 0.000050, simclr_loss: 4.716882, grp_loss: 1.453436\n","Test set: Average loss: 0.18, Accuracy: 144/150 F1 (96.00%), F1 score: 95.45%\n","Test set: Average loss: 0.15, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.875566, loss_c: 0.004228, simclr_loss: 4.717834, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.890120, loss_c: 0.045369, simclr_loss: 4.718657, grp_loss: 0.970123\n","Test set: Average loss: 0.22, Accuracy: 148/150 F1 (98.67%), F1 score: 90.85%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.945007, loss_c: 0.099506, simclr_loss: 4.718623, grp_loss: 0.971010\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.850822, loss_c: 0.000285, simclr_loss: 4.718780, grp_loss: 0.975418\n","Test set: Average loss: 0.30, Accuracy: 142/150 F1 (94.67%), F1 score: 86.26%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.901939, loss_c: 0.038147, simclr_loss: 4.722571, grp_loss: 0.973507\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.847500, loss_c: 0.012077, simclr_loss: 4.716121, grp_loss: 0.970940\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 95.42%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.855623, loss_c: 0.013294, simclr_loss: 4.717857, grp_loss: 0.970900\n","Test set: Average loss: 0.17, Accuracy: 145/150 F1 (96.67%), F1 score: 91.86%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.869339, loss_c: 0.022026, simclr_loss: 4.719372, grp_loss: 0.969826\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 95.49%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.849226, loss_c: 0.003337, simclr_loss: 4.718874, grp_loss: 0.970396\n","Test set: Average loss: 0.04, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.852266, loss_c: 0.000034, simclr_loss: 4.720564, grp_loss: 0.969976\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.09, Accuracy: 149/150 F1 (99.33%), F1 score: 95.41%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 95.413410\n","Ep: 0 lr: 0.01, loss_all: 18.879425, loss_c: 0.000054, simclr_loss: 4.719843, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.312038, loss_c: 0.000049, simclr_loss: 4.714761, grp_loss: 1.452946\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.387981, loss_c: 0.050905, simclr_loss: 4.720611, grp_loss: 1.454634\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.317120, loss_c: 0.000319, simclr_loss: 4.716005, grp_loss: 1.452780\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.328621, loss_c: 0.001733, simclr_loss: 4.718324, grp_loss: 1.453592\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.37%\n","Ep: 50 lr: 0.005, loss_all: 20.329788, loss_c: 0.000008, simclr_loss: 4.718894, grp_loss: 1.454203\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.316961, loss_c: 0.002576, simclr_loss: 4.715388, grp_loss: 1.452831\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.310999, loss_c: 0.000058, simclr_loss: 4.714540, grp_loss: 1.452782\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.324348, loss_c: 0.000596, simclr_loss: 4.717532, grp_loss: 1.453623\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319656, loss_c: 0.000135, simclr_loss: 4.716373, grp_loss: 1.454028\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Ep: 0 lr: 0.01, loss_all: 18.870764, loss_c: 0.000502, simclr_loss: 4.717566, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.338600, loss_c: 0.012933, simclr_loss: 4.717555, grp_loss: 1.455449\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.315662, loss_c: 0.000155, simclr_loss: 4.715601, grp_loss: 1.453104\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 95.35%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.315147, loss_c: 0.000045, simclr_loss: 4.715492, grp_loss: 1.453135\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.52%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.320614, loss_c: 0.000032, simclr_loss: 4.716852, grp_loss: 1.453172\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.312134, loss_c: 0.000025, simclr_loss: 4.714767, grp_loss: 1.453038\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.35%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.315727, loss_c: 0.000055, simclr_loss: 4.715724, grp_loss: 1.452776\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 95.48%\n","Model saved at step 60 with best accuracy 97.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.312586, loss_c: 0.000089, simclr_loss: 4.714932, grp_loss: 1.452768\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.337471, loss_c: 0.001372, simclr_loss: 4.720443, grp_loss: 1.454327\n","Test set: Average loss: 0.26, Accuracy: 146/150 F1 (97.33%), F1 score: 91.05%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.310867, loss_c: 0.000017, simclr_loss: 4.714496, grp_loss: 1.452866\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.21, Accuracy: 146/150 F1 (97.33%), F1 score: 90.08%\n","Model saved at step 99 with best accuracy 97.33333333333333\n","Ep: 0 lr: 0.01, loss_all: 18.872156, loss_c: 0.000419, simclr_loss: 4.717935, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.871447, loss_c: 0.033551, simclr_loss: 4.716787, grp_loss: 0.970748\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.831673, loss_c: 0.000755, simclr_loss: 4.714440, grp_loss: 0.973156\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 95.47%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.845713, loss_c: 0.000106, simclr_loss: 4.718697, grp_loss: 0.970819\n","Test set: Average loss: 0.12, Accuracy: 143/150 F1 (95.33%), F1 score: 95.54%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.851786, loss_c: 0.012176, simclr_loss: 4.717255, grp_loss: 0.970591\n","Test set: Average loss: 0.29, Accuracy: 143/150 F1 (95.33%), F1 score: 90.59%\n","Ep: 50 lr: 0.005, loss_all: 19.836096, loss_c: 0.000079, simclr_loss: 4.714929, grp_loss: 0.976304\n","Test set: Average loss: 0.28, Accuracy: 141/150 F1 (94.00%), F1 score: 90.34%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.844595, loss_c: 0.000547, simclr_loss: 4.718324, grp_loss: 0.970753\n","Test set: Average loss: 0.27, Accuracy: 138/150 F1 (92.00%), F1 score: 87.27%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.845268, loss_c: 0.009851, simclr_loss: 4.716314, grp_loss: 0.970159\n","Test set: Average loss: 0.16, Accuracy: 141/150 F1 (94.00%), F1 score: 95.30%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.843468, loss_c: 0.000302, simclr_loss: 4.717578, grp_loss: 0.972850\n","Test set: Average loss: 0.17, Accuracy: 141/150 F1 (94.00%), F1 score: 95.41%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.838129, loss_c: 0.003924, simclr_loss: 4.715370, grp_loss: 0.972726\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.23, Accuracy: 141/150 F1 (94.00%), F1 score: 95.43%\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.858578, loss_c: 0.000003, simclr_loss: 4.714643, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.319120, loss_c: 0.000025, simclr_loss: 4.716463, grp_loss: 1.453244\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.51%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.312492, loss_c: 0.000005, simclr_loss: 4.714931, grp_loss: 1.452767\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.430923, loss_c: 0.116080, simclr_loss: 4.715519, grp_loss: 1.452768\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.338505, loss_c: 0.008515, simclr_loss: 4.718925, grp_loss: 1.454293\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.317995, loss_c: 0.000002, simclr_loss: 4.716306, grp_loss: 1.452771\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.322001, loss_c: 0.000023, simclr_loss: 4.717298, grp_loss: 1.452786\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.314209, loss_c: 0.000008, simclr_loss: 4.715276, grp_loss: 1.453095\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318457, loss_c: 0.000005, simclr_loss: 4.716420, grp_loss: 1.452775\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.311050, loss_c: 0.000006, simclr_loss: 4.714552, grp_loss: 1.452836\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.860546, loss_c: 0.000611, simclr_loss: 4.714984, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.314213, loss_c: 0.000449, simclr_loss: 4.715142, grp_loss: 1.453197\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 95.36%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.333817, loss_c: 0.000008, simclr_loss: 4.719818, grp_loss: 1.454535\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.325075, loss_c: 0.000095, simclr_loss: 4.717593, grp_loss: 1.454606\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.327925, loss_c: 0.000150, simclr_loss: 4.718740, grp_loss: 1.452813\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.324486, loss_c: 0.001081, simclr_loss: 4.717006, grp_loss: 1.455380\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.314320, loss_c: 0.000025, simclr_loss: 4.715329, grp_loss: 1.452977\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.311350, loss_c: 0.000367, simclr_loss: 4.714551, grp_loss: 1.452779\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.320627, loss_c: 0.000003, simclr_loss: 4.716798, grp_loss: 1.453432\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.325483, loss_c: 0.000217, simclr_loss: 4.717814, grp_loss: 1.454008\n","Test set: Average loss: 0.17, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 95.49%\n","Ep: 0 lr: 0.01, loss_all: 18.958652, loss_c: 0.089481, simclr_loss: 4.717293, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.854370, loss_c: 0.006171, simclr_loss: 4.717479, grp_loss: 0.978285\n","Test set: Average loss: 0.32, Accuracy: 143/150 F1 (95.33%), F1 score: 91.25%\n","Model saved at step 10 with best accuracy 95.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.845425, loss_c: 0.006923, simclr_loss: 4.717146, grp_loss: 0.969917\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.829033, loss_c: 0.000830, simclr_loss: 4.714495, grp_loss: 0.970223\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 95.53%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.853451, loss_c: 0.007908, simclr_loss: 4.717870, grp_loss: 0.974063\n","Test set: Average loss: 0.13, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.865463, loss_c: 0.000198, simclr_loss: 4.723777, grp_loss: 0.970157\n","Test set: Average loss: 0.14, Accuracy: 145/150 F1 (96.67%), F1 score: 95.53%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.849833, loss_c: 0.000915, simclr_loss: 4.719657, grp_loss: 0.970287\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.839975, loss_c: 0.000053, simclr_loss: 4.716912, grp_loss: 0.972272\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.840700, loss_c: 0.003602, simclr_loss: 4.716718, grp_loss: 0.970228\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.830599, loss_c: 0.000108, simclr_loss: 4.714971, grp_loss: 0.970610\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.36%\n","Model saved at step 99 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.988089, loss_c: 0.124465, simclr_loss: 4.715906, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.323704, loss_c: 0.000083, simclr_loss: 4.717293, grp_loss: 1.454450\n","Test set: Average loss: 0.14, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 96.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.311916, loss_c: 0.000019, simclr_loss: 4.714739, grp_loss: 1.452941\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 97.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.321192, loss_c: 0.001972, simclr_loss: 4.716414, grp_loss: 1.453564\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.335564, loss_c: 0.000025, simclr_loss: 4.720643, grp_loss: 1.452967\n","Test set: Average loss: 0.14, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.319881, loss_c: 0.000002, simclr_loss: 4.716361, grp_loss: 1.454436\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.314102, loss_c: 0.000031, simclr_loss: 4.715325, grp_loss: 1.452771\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.333542, loss_c: 0.000002, simclr_loss: 4.720124, grp_loss: 1.453042\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.319719, loss_c: 0.000723, simclr_loss: 4.716178, grp_loss: 1.454284\n","Test set: Average loss: 0.22, Accuracy: 147/150 F1 (98.00%), F1 score: 95.61%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.325384, loss_c: 0.000024, simclr_loss: 4.717806, grp_loss: 1.454135\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 18.899183, loss_c: 0.030247, simclr_loss: 4.717234, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.336008, loss_c: 0.000583, simclr_loss: 4.719485, grp_loss: 1.457485\n","Test set: Average loss: 0.31, Accuracy: 145/150 F1 (96.67%), F1 score: 95.44%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.316956, loss_c: 0.000025, simclr_loss: 4.715916, grp_loss: 1.453268\n","Test set: Average loss: 0.24, Accuracy: 145/150 F1 (96.67%), F1 score: 90.80%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.380707, loss_c: 0.062757, simclr_loss: 4.716198, grp_loss: 1.453155\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 96.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.326870, loss_c: 0.000975, simclr_loss: 4.718224, grp_loss: 1.453001\n","Test set: Average loss: 0.22, Accuracy: 145/150 F1 (96.67%), F1 score: 90.76%\n","Model saved at step 40 with best accuracy 96.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.312262, loss_c: 0.000045, simclr_loss: 4.714844, grp_loss: 1.452841\n","Test set: Average loss: 0.20, Accuracy: 145/150 F1 (96.67%), F1 score: 95.40%\n","Model saved at step 50 with best accuracy 96.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.330849, loss_c: 0.000418, simclr_loss: 4.718815, grp_loss: 1.455172\n","Test set: Average loss: 0.19, Accuracy: 145/150 F1 (96.67%), F1 score: 95.67%\n","Model saved at step 60 with best accuracy 96.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.310270, loss_c: 0.000044, simclr_loss: 4.714356, grp_loss: 1.452801\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.327957, loss_c: 0.000009, simclr_loss: 4.718168, grp_loss: 1.455274\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.328629, loss_c: 0.000735, simclr_loss: 4.718719, grp_loss: 1.453020\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.66666666666667\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.66666666666667\n","Ep: 0 lr: 0.01, loss_all: 18.861450, loss_c: 0.000374, simclr_loss: 4.715269, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.868130, loss_c: 0.009028, simclr_loss: 4.720722, grp_loss: 0.976213\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.842667, loss_c: 0.000048, simclr_loss: 4.718198, grp_loss: 0.969827\n","Test set: Average loss: 0.09, Accuracy: 145/150 F1 (96.67%), F1 score: 95.51%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.833002, loss_c: 0.000453, simclr_loss: 4.715231, grp_loss: 0.971626\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.864964, loss_c: 0.012805, simclr_loss: 4.720376, grp_loss: 0.970652\n","Test set: Average loss: 0.04, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.864788, loss_c: 0.000043, simclr_loss: 4.722160, grp_loss: 0.976102\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.845873, loss_c: 0.000180, simclr_loss: 4.718711, grp_loss: 0.970848\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 90.87%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.837423, loss_c: 0.000050, simclr_loss: 4.716610, grp_loss: 0.970933\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 91.25%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.856962, loss_c: 0.004998, simclr_loss: 4.719628, grp_loss: 0.973453\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.857924, loss_c: 0.007360, simclr_loss: 4.719006, grp_loss: 0.974539\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Test set: Average loss: 0.08, Accuracy: 146/150 F1 (97.33%), F1 score: 95.53%\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.865782, loss_c: 0.004817, simclr_loss: 4.715241, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.393263, loss_c: 0.047944, simclr_loss: 4.722968, grp_loss: 1.453447\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 95.45%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.315672, loss_c: 0.000024, simclr_loss: 4.715719, grp_loss: 1.452770\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.325378, loss_c: 0.001958, simclr_loss: 4.717662, grp_loss: 1.452774\n","Test set: Average loss: 0.24, Accuracy: 145/150 F1 (96.67%), F1 score: 95.49%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.317675, loss_c: 0.000959, simclr_loss: 4.715375, grp_loss: 1.455216\n","Test set: Average loss: 0.11, Accuracy: 148/150 F1 (98.67%), F1 score: 95.59%\n","Model saved at step 40 with best accuracy 98.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.319408, loss_c: 0.000063, simclr_loss: 4.716426, grp_loss: 1.453643\n","Test set: Average loss: 0.07, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.330589, loss_c: 0.000019, simclr_loss: 4.719397, grp_loss: 1.452984\n","Test set: Average loss: 0.11, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.333456, loss_c: 0.000004, simclr_loss: 4.720129, grp_loss: 1.452939\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.321327, loss_c: 0.000032, simclr_loss: 4.716937, grp_loss: 1.453547\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.315388, loss_c: 0.000653, simclr_loss: 4.715457, grp_loss: 1.452907\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.864086, loss_c: 0.002986, simclr_loss: 4.715275, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.333817, loss_c: 0.000042, simclr_loss: 4.720241, grp_loss: 1.452810\n","Test set: Average loss: 0.24, Accuracy: 147/150 F1 (98.00%), F1 score: 90.50%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.310762, loss_c: 0.000366, simclr_loss: 4.714398, grp_loss: 1.452802\n","Test set: Average loss: 0.31, Accuracy: 146/150 F1 (97.33%), F1 score: 90.91%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.317827, loss_c: 0.000037, simclr_loss: 4.716204, grp_loss: 1.452976\n","Test set: Average loss: 0.19, Accuracy: 147/150 F1 (98.00%), F1 score: 90.85%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.342716, loss_c: 0.000004, simclr_loss: 4.722081, grp_loss: 1.454387\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 20.322872, loss_c: 0.000033, simclr_loss: 4.717278, grp_loss: 1.453726\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 95.47%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.311165, loss_c: 0.000162, simclr_loss: 4.714553, grp_loss: 1.452790\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.40%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.324364, loss_c: 0.000001, simclr_loss: 4.717854, grp_loss: 1.452950\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.316576, loss_c: 0.000061, simclr_loss: 4.715916, grp_loss: 1.452852\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.323589, loss_c: 0.000002, simclr_loss: 4.717196, grp_loss: 1.454806\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Ep: 0 lr: 0.01, loss_all: 18.890862, loss_c: 0.028430, simclr_loss: 4.715608, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.866581, loss_c: 0.009858, simclr_loss: 4.721301, grp_loss: 0.971522\n","Test set: Average loss: 0.53, Accuracy: 124/150 F1 (82.67%), F1 score: 87.15%\n","Model saved at step 10 with best accuracy 82.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.860472, loss_c: 0.006900, simclr_loss: 4.719774, grp_loss: 0.974474\n","Test set: Average loss: 0.21, Accuracy: 143/150 F1 (95.33%), F1 score: 86.82%\n","Model saved at step 20 with best accuracy 95.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.842188, loss_c: 0.000306, simclr_loss: 4.714932, grp_loss: 0.982154\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 90.80%\n","Model saved at step 30 with best accuracy 97.33333333333333\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.853952, loss_c: 0.014075, simclr_loss: 4.717124, grp_loss: 0.971382\n","Test set: Average loss: 0.24, Accuracy: 143/150 F1 (95.33%), F1 score: 91.23%\n","Ep: 50 lr: 0.005, loss_all: 19.848606, loss_c: 0.008176, simclr_loss: 4.715723, grp_loss: 0.977539\n","Test set: Average loss: 0.45, Accuracy: 136/150 F1 (90.67%), F1 score: 83.09%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.885990, loss_c: 0.040341, simclr_loss: 4.717455, grp_loss: 0.975831\n","Test set: Average loss: 0.04, Accuracy: 148/150 F1 (98.67%), F1 score: 96.10%\n","Model saved at step 60 with best accuracy 98.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.835861, loss_c: 0.001265, simclr_loss: 4.714933, grp_loss: 0.974865\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.832947, loss_c: 0.000759, simclr_loss: 4.715354, grp_loss: 0.970770\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.836697, loss_c: 0.000184, simclr_loss: 4.716571, grp_loss: 0.970228\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 99.33333333333333\n","Test set: Average loss: 0.02, Accuracy: 150/150 F1 (100.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 100.0\n","Test set: Average loss: 0.01, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.868467, loss_c: 0.001625, simclr_loss: 4.716711, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.323654, loss_c: 0.000353, simclr_loss: 4.716918, grp_loss: 1.455631\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.323637, loss_c: 0.000002, simclr_loss: 4.717502, grp_loss: 1.453629\n","Test set: Average loss: 0.04, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 99.33333333333333\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.323166, loss_c: 0.000013, simclr_loss: 4.717571, grp_loss: 1.452867\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.349127, loss_c: 0.010312, simclr_loss: 4.720574, grp_loss: 1.456518\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 95.35%\n","Ep: 50 lr: 0.005, loss_all: 20.333397, loss_c: 0.000445, simclr_loss: 4.719754, grp_loss: 1.453936\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.316528, loss_c: 0.000084, simclr_loss: 4.715918, grp_loss: 1.452773\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.320162, loss_c: 0.000054, simclr_loss: 4.716702, grp_loss: 1.453298\n","Test set: Average loss: 0.19, Accuracy: 147/150 F1 (98.00%), F1 score: 91.15%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.330336, loss_c: 0.000144, simclr_loss: 4.719043, grp_loss: 1.454019\n","Test set: Average loss: 0.21, Accuracy: 145/150 F1 (96.67%), F1 score: 95.34%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.335039, loss_c: 0.000105, simclr_loss: 4.720317, grp_loss: 1.453667\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Ep: 0 lr: 0.01, loss_all: 18.865637, loss_c: 0.000063, simclr_loss: 4.716393, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.323414, loss_c: 0.000493, simclr_loss: 4.717025, grp_loss: 1.454823\n","Test set: Average loss: 0.30, Accuracy: 140/150 F1 (93.33%), F1 score: 91.15%\n","Model saved at step 10 with best accuracy 93.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.316370, loss_c: 0.000138, simclr_loss: 4.715711, grp_loss: 1.453390\n","Test set: Average loss: 0.16, Accuracy: 147/150 F1 (98.00%), F1 score: 90.94%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.313520, loss_c: 0.000866, simclr_loss: 4.714964, grp_loss: 1.452797\n","Test set: Average loss: 0.09, Accuracy: 148/150 F1 (98.67%), F1 score: 95.38%\n","Model saved at step 30 with best accuracy 98.66666666666667\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.317667, loss_c: 0.000078, simclr_loss: 4.716164, grp_loss: 1.452934\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 99.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.317982, loss_c: 0.000004, simclr_loss: 4.716302, grp_loss: 1.452769\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.327154, loss_c: 0.000352, simclr_loss: 4.718386, grp_loss: 1.453258\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.312790, loss_c: 0.000016, simclr_loss: 4.714904, grp_loss: 1.453158\n","Test set: Average loss: 0.18, Accuracy: 147/150 F1 (98.00%), F1 score: 95.29%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.329580, loss_c: 0.012222, simclr_loss: 4.715823, grp_loss: 1.454065\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.327606, loss_c: 0.000134, simclr_loss: 4.718674, grp_loss: 1.452778\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.865828, loss_c: 0.000959, simclr_loss: 4.716217, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.859503, loss_c: 0.008366, simclr_loss: 4.720070, grp_loss: 0.970857\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.61%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.882706, loss_c: 0.029414, simclr_loss: 4.719075, grp_loss: 0.976992\n","Test set: Average loss: 0.23, Accuracy: 138/150 F1 (92.00%), F1 score: 83.64%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.877535, loss_c: 0.027533, simclr_loss: 4.719493, grp_loss: 0.972031\n","Test set: Average loss: 0.14, Accuracy: 140/150 F1 (93.33%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.837296, loss_c: 0.000073, simclr_loss: 4.716215, grp_loss: 0.972362\n","Test set: Average loss: 0.15, Accuracy: 141/150 F1 (94.00%), F1 score: 95.53%\n","Ep: 50 lr: 0.005, loss_all: 19.850401, loss_c: 0.002882, simclr_loss: 4.719225, grp_loss: 0.970620\n","Test set: Average loss: 0.14, Accuracy: 143/150 F1 (95.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.836065, loss_c: 0.001166, simclr_loss: 4.715933, grp_loss: 0.971167\n","Test set: Average loss: 0.08, Accuracy: 145/150 F1 (96.67%), F1 score: 95.44%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.853947, loss_c: 0.000887, simclr_loss: 4.719592, grp_loss: 0.974692\n","Test set: Average loss: 0.06, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.857449, loss_c: 0.013387, simclr_loss: 4.718333, grp_loss: 0.970731\n","Test set: Average loss: 0.04, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 80 with best accuracy 98.0\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.845272, loss_c: 0.000329, simclr_loss: 4.718122, grp_loss: 0.972456\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 99 with best accuracy 98.0\n","Test set: Average loss: 0.03, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.862324, loss_c: 0.000121, simclr_loss: 4.715550, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.372133, loss_c: 0.061002, simclr_loss: 4.714584, grp_loss: 1.452795\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.324739, loss_c: 0.000007, simclr_loss: 4.717359, grp_loss: 1.455296\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 95.44%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.311739, loss_c: 0.000027, simclr_loss: 4.714735, grp_loss: 1.452772\n","Test set: Average loss: 0.24, Accuracy: 146/150 F1 (97.33%), F1 score: 95.24%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.333435, loss_c: 0.001007, simclr_loss: 4.719521, grp_loss: 1.454345\n","Test set: Average loss: 0.38, Accuracy: 146/150 F1 (97.33%), F1 score: 95.31%\n","Ep: 50 lr: 0.005, loss_all: 20.318962, loss_c: 0.000003, simclr_loss: 4.716432, grp_loss: 1.453229\n","Test set: Average loss: 0.22, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.313757, loss_c: 0.000008, simclr_loss: 4.715243, grp_loss: 1.452777\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.317314, loss_c: 0.000010, simclr_loss: 4.715758, grp_loss: 1.454270\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311823, loss_c: 0.000136, simclr_loss: 4.714703, grp_loss: 1.452875\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.320696, loss_c: 0.000003, simclr_loss: 4.716968, grp_loss: 1.452823\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.878040, loss_c: 0.000010, simclr_loss: 4.719508, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.336315, loss_c: 0.022391, simclr_loss: 4.715280, grp_loss: 1.452806\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.314819, loss_c: 0.000010, simclr_loss: 4.715245, grp_loss: 1.453830\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.329294, loss_c: 0.000831, simclr_loss: 4.718303, grp_loss: 1.455252\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.315083, loss_c: 0.000151, simclr_loss: 4.715537, grp_loss: 1.452783\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.318283, loss_c: 0.002298, simclr_loss: 4.715788, grp_loss: 1.452832\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 95.41%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.333452, loss_c: 0.003374, simclr_loss: 4.719235, grp_loss: 1.453137\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.322960, loss_c: 0.000161, simclr_loss: 4.717193, grp_loss: 1.454025\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 95.39%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.318851, loss_c: 0.000116, simclr_loss: 4.716480, grp_loss: 1.452814\n","Test set: Average loss: 0.20, Accuracy: 144/150 F1 (96.00%), F1 score: 95.44%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.318539, loss_c: 0.000038, simclr_loss: 4.716428, grp_loss: 1.452789\n","Test set: Average loss: 0.10, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 95.34%\n","Ep: 0 lr: 0.01, loss_all: 18.948145, loss_c: 0.079805, simclr_loss: 4.717085, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.904882, loss_c: 0.053444, simclr_loss: 4.720196, grp_loss: 0.970656\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 90.75%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.839142, loss_c: 0.000119, simclr_loss: 4.717283, grp_loss: 0.969891\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.841946, loss_c: 0.001087, simclr_loss: 4.717384, grp_loss: 0.971320\n","Test set: Average loss: 0.05, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.842197, loss_c: 0.012385, simclr_loss: 4.714935, grp_loss: 0.970074\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 99.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 19.845304, loss_c: 0.010945, simclr_loss: 4.716014, grp_loss: 0.970304\n","Test set: Average loss: 0.03, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.836893, loss_c: 0.002619, simclr_loss: 4.716102, grp_loss: 0.969867\n","Test set: Average loss: 0.06, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.831226, loss_c: 0.000090, simclr_loss: 4.715300, grp_loss: 0.969938\n","Test set: Average loss: 0.09, Accuracy: 146/150 F1 (97.33%), F1 score: 90.53%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.840546, loss_c: 0.000713, simclr_loss: 4.716949, grp_loss: 0.972036\n","Test set: Average loss: 0.11, Accuracy: 145/150 F1 (96.67%), F1 score: 95.55%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.852528, loss_c: 0.007270, simclr_loss: 4.718646, grp_loss: 0.970675\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 95.44%\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Test set: Average loss: 0.05, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Federated Best acc test 98.666667\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.860579, loss_c: 0.000230, simclr_loss: 4.715087, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.328270, loss_c: 0.000862, simclr_loss: 4.718656, grp_loss: 1.452786\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 95.41%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.324638, loss_c: 0.000031, simclr_loss: 4.717960, grp_loss: 1.452767\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.313725, loss_c: 0.000001, simclr_loss: 4.715236, grp_loss: 1.452781\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.65%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.311115, loss_c: 0.000004, simclr_loss: 4.714579, grp_loss: 1.452797\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.322798, loss_c: 0.000028, simclr_loss: 4.717499, grp_loss: 1.452774\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.0\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.327652, loss_c: 0.000060, simclr_loss: 4.718674, grp_loss: 1.452899\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 60 with best accuracy 98.0\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.310991, loss_c: 0.000004, simclr_loss: 4.714550, grp_loss: 1.452786\n","Test set: Average loss: 0.21, Accuracy: 147/150 F1 (98.00%), F1 score: 95.47%\n","Model saved at step 70 with best accuracy 98.0\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.315992, loss_c: 0.000761, simclr_loss: 4.715602, grp_loss: 1.452824\n","Test set: Average loss: 0.34, Accuracy: 146/150 F1 (97.33%), F1 score: 91.05%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.319546, loss_c: 0.000007, simclr_loss: 4.716690, grp_loss: 1.452777\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.37, Accuracy: 147/150 F1 (98.00%), F1 score: 90.51%\n","Model saved at step 99 with best accuracy 98.0\n","Ep: 0 lr: 0.01, loss_all: 18.878021, loss_c: 0.000199, simclr_loss: 4.719456, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.315002, loss_c: 0.000023, simclr_loss: 4.715455, grp_loss: 1.453159\n","Test set: Average loss: 0.48, Accuracy: 144/150 F1 (96.00%), F1 score: 91.31%\n","Model saved at step 10 with best accuracy 96.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.315720, loss_c: 0.000067, simclr_loss: 4.715721, grp_loss: 1.452769\n","Test set: Average loss: 0.22, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 96.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.312462, loss_c: 0.000018, simclr_loss: 4.714880, grp_loss: 1.452924\n","Test set: Average loss: 0.34, Accuracy: 144/150 F1 (96.00%), F1 score: 95.50%\n","Model saved at step 30 with best accuracy 96.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.318420, loss_c: 0.000022, simclr_loss: 4.716386, grp_loss: 1.452856\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 97.33333333333333\n","Ep: 50 lr: 0.005, loss_all: 20.337807, loss_c: 0.000698, simclr_loss: 4.720487, grp_loss: 1.455160\n","Test set: Average loss: 0.27, Accuracy: 146/150 F1 (97.33%), F1 score: 95.51%\n","Model saved at step 50 with best accuracy 97.33333333333333\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.333733, loss_c: 0.000001, simclr_loss: 4.719720, grp_loss: 1.454851\n","Test set: Average loss: 0.19, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Model saved at step 60 with best accuracy 97.33333333333333\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.325487, loss_c: 0.000006, simclr_loss: 4.716644, grp_loss: 1.458904\n","Test set: Average loss: 0.13, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 70 with best accuracy 97.33333333333333\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.329205, loss_c: 0.000030, simclr_loss: 4.718060, grp_loss: 1.456934\n","Test set: Average loss: 0.29, Accuracy: 146/150 F1 (97.33%), F1 score: 95.31%\n","Model saved at step 80 with best accuracy 97.33333333333333\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.318676, loss_c: 0.000257, simclr_loss: 4.716352, grp_loss: 1.453012\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 97.33333333333333\n","Test set: Average loss: 0.19, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.865593, loss_c: 0.004306, simclr_loss: 4.715322, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.846693, loss_c: 0.000248, simclr_loss: 4.717931, grp_loss: 0.974719\n","Test set: Average loss: 0.02, Accuracy: 149/150 F1 (99.33%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 99.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.836954, loss_c: 0.000418, simclr_loss: 4.716556, grp_loss: 0.970314\n","Test set: Average loss: 0.10, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.852871, loss_c: 0.001930, simclr_loss: 4.719970, grp_loss: 0.971061\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.836531, loss_c: 0.000495, simclr_loss: 4.716244, grp_loss: 0.971058\n","Test set: Average loss: 0.72, Accuracy: 129/150 F1 (86.00%), F1 score: 86.22%\n","Ep: 50 lr: 0.005, loss_all: 19.872974, loss_c: 0.026965, simclr_loss: 4.717619, grp_loss: 0.975532\n","Test set: Average loss: 0.49, Accuracy: 134/150 F1 (89.33%), F1 score: 81.77%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.857384, loss_c: 0.018142, simclr_loss: 4.716960, grp_loss: 0.971401\n","Test set: Average loss: 0.32, Accuracy: 137/150 F1 (91.33%), F1 score: 90.89%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.833830, loss_c: 0.001568, simclr_loss: 4.715560, grp_loss: 0.970022\n","Test set: Average loss: 0.13, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.838085, loss_c: 0.004501, simclr_loss: 4.715760, grp_loss: 0.970544\n","Test set: Average loss: 0.30, Accuracy: 137/150 F1 (91.33%), F1 score: 90.61%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.865944, loss_c: 0.026138, simclr_loss: 4.716860, grp_loss: 0.972365\n","Test set: Average loss: 0.43, Accuracy: 133/150 F1 (88.67%), F1 score: 80.80%\n","Test set: Average loss: 0.39, Accuracy: 137/150 F1 (91.33%), F1 score: 91.19%\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Federated Best acc test 98.000000\n","Federated Best fscore test 100.000000\n","Ep: 0 lr: 0.01, loss_all: 18.858543, loss_c: 0.000591, simclr_loss: 4.714488, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.325184, loss_c: 0.000226, simclr_loss: 4.718021, grp_loss: 1.452874\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 95.45%\n","Model saved at step 10 with best accuracy 97.33333333333333\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.313789, loss_c: 0.000070, simclr_loss: 4.715226, grp_loss: 1.452814\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.332325, loss_c: 0.000002, simclr_loss: 4.719726, grp_loss: 1.453420\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.326420, loss_c: 0.000015, simclr_loss: 4.718144, grp_loss: 1.453827\n","Test set: Average loss: 0.14, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.335316, loss_c: 0.000026, simclr_loss: 4.720630, grp_loss: 1.452771\n","Test set: Average loss: 0.16, Accuracy: 146/150 F1 (97.33%), F1 score: 95.29%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.320246, loss_c: 0.000028, simclr_loss: 4.716840, grp_loss: 1.452859\n","Test set: Average loss: 0.31, Accuracy: 145/150 F1 (96.67%), F1 score: 86.70%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.327770, loss_c: 0.000001, simclr_loss: 4.718738, grp_loss: 1.452818\n","Test set: Average loss: 0.23, Accuracy: 145/150 F1 (96.67%), F1 score: 95.44%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.326710, loss_c: 0.000080, simclr_loss: 4.717939, grp_loss: 1.454873\n","Test set: Average loss: 0.22, Accuracy: 145/150 F1 (96.67%), F1 score: 95.53%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.312368, loss_c: 0.000239, simclr_loss: 4.714798, grp_loss: 1.452938\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 90 with best accuracy 98.0\n","Test set: Average loss: 0.15, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.864492, loss_c: 0.000017, simclr_loss: 4.716119, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.319881, loss_c: 0.000380, simclr_loss: 4.716338, grp_loss: 1.454149\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 96.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.347530, loss_c: 0.000029, simclr_loss: 4.723539, grp_loss: 1.453345\n","Test set: Average loss: 0.30, Accuracy: 145/150 F1 (96.67%), F1 score: 89.81%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.313641, loss_c: 0.000003, simclr_loss: 4.715117, grp_loss: 1.453168\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.32%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.311724, loss_c: 0.000011, simclr_loss: 4.714735, grp_loss: 1.452774\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 95.25%\n","Model saved at step 40 with best accuracy 98.0\n","Ep: 50 lr: 0.005, loss_all: 20.310398, loss_c: 0.000253, simclr_loss: 4.714344, grp_loss: 1.452771\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 50 with best accuracy 98.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.327911, loss_c: 0.000330, simclr_loss: 4.717231, grp_loss: 1.458659\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 95.35%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.323042, loss_c: 0.000204, simclr_loss: 4.717508, grp_loss: 1.452804\n","Test set: Average loss: 0.09, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.311317, loss_c: 0.000009, simclr_loss: 4.714571, grp_loss: 1.453021\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.313934, loss_c: 0.000165, simclr_loss: 4.714759, grp_loss: 1.454733\n","Test set: Average loss: 0.12, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 0 lr: 0.01, loss_all: 18.862549, loss_c: 0.000403, simclr_loss: 4.715537, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.835993, loss_c: 0.002337, simclr_loss: 4.715848, grp_loss: 0.970265\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 90.79%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.883326, loss_c: 0.033195, simclr_loss: 4.719759, grp_loss: 0.971094\n","Test set: Average loss: 0.30, Accuracy: 139/150 F1 (92.67%), F1 score: 90.71%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.881130, loss_c: 0.001398, simclr_loss: 4.726537, grp_loss: 0.973585\n","Test set: Average loss: 0.12, Accuracy: 146/150 F1 (97.33%), F1 score: 91.37%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.846731, loss_c: 0.005136, simclr_loss: 4.716833, grp_loss: 0.974262\n","Test set: Average loss: 0.06, Accuracy: 145/150 F1 (96.67%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 19.863300, loss_c: 0.028371, simclr_loss: 4.716257, grp_loss: 0.969902\n","Test set: Average loss: 0.44, Accuracy: 135/150 F1 (90.00%), F1 score: 80.29%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.855305, loss_c: 0.000151, simclr_loss: 4.719640, grp_loss: 0.976592\n","Test set: Average loss: 0.42, Accuracy: 136/150 F1 (90.67%), F1 score: 69.81%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.858137, loss_c: 0.002913, simclr_loss: 4.721085, grp_loss: 0.970887\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 95.40%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.835201, loss_c: 0.001265, simclr_loss: 4.715619, grp_loss: 0.971460\n","Test set: Average loss: 0.07, Accuracy: 146/150 F1 (97.33%), F1 score: 95.47%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 19.865856, loss_c: 0.003836, simclr_loss: 4.718915, grp_loss: 0.986359\n","Test set: Average loss: 0.11, Accuracy: 144/150 F1 (96.00%), F1 score: 90.07%\n","Test set: Average loss: 0.07, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Test set: Average loss: 0.11, Accuracy: 149/150 F1 (99.33%), F1 score: 95.48%\n","Federated Best acc test 99.333333\n","Federated Best fscore test 95.475348\n","Ep: 0 lr: 0.01, loss_all: 18.857399, loss_c: 0.000000, simclr_loss: 4.714350, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.332397, loss_c: 0.000002, simclr_loss: 4.719906, grp_loss: 1.452772\n","Test set: Average loss: 0.08, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.0\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.322258, loss_c: 0.000071, simclr_loss: 4.717346, grp_loss: 1.452802\n","Test set: Average loss: 0.11, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 20 with best accuracy 98.0\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.322250, loss_c: 0.000008, simclr_loss: 4.717358, grp_loss: 1.452813\n","Test set: Average loss: 0.15, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Model saved at step 30 with best accuracy 98.0\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.318634, loss_c: 0.000001, simclr_loss: 4.715458, grp_loss: 1.456801\n","Test set: Average loss: 0.27, Accuracy: 146/150 F1 (97.33%), F1 score: 95.37%\n","Ep: 50 lr: 0.005, loss_all: 20.316397, loss_c: 0.000003, simclr_loss: 4.715906, grp_loss: 1.452768\n","Test set: Average loss: 0.25, Accuracy: 146/150 F1 (97.33%), F1 score: 95.41%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.313290, loss_c: 0.000004, simclr_loss: 4.715130, grp_loss: 1.452767\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.310963, loss_c: 0.000012, simclr_loss: 4.714539, grp_loss: 1.452795\n","Test set: Average loss: 0.42, Accuracy: 146/150 F1 (97.33%), F1 score: 90.91%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.310169, loss_c: 0.000004, simclr_loss: 4.714348, grp_loss: 1.452771\n","Test set: Average loss: 0.17, Accuracy: 146/150 F1 (97.33%), F1 score: 100.00%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.335882, loss_c: 0.000001, simclr_loss: 4.720777, grp_loss: 1.452774\n","Test set: Average loss: 0.25, Accuracy: 146/150 F1 (97.33%), F1 score: 95.40%\n","Test set: Average loss: 0.26, Accuracy: 145/150 F1 (96.67%), F1 score: 95.21%\n","Ep: 0 lr: 0.01, loss_all: 18.860620, loss_c: 0.002433, simclr_loss: 4.714547, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 20.337852, loss_c: 0.000014, simclr_loss: 4.720776, grp_loss: 1.454734\n","Test set: Average loss: 0.06, Accuracy: 148/150 F1 (98.67%), F1 score: 100.00%\n","Model saved at step 10 with best accuracy 98.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 20.318632, loss_c: 0.001662, simclr_loss: 4.716004, grp_loss: 1.452954\n","Test set: Average loss: 0.15, Accuracy: 146/150 F1 (97.33%), F1 score: 95.42%\n","Ep: 30 lr: 0.007938926261462366, loss_all: 20.315697, loss_c: 0.000008, simclr_loss: 4.715719, grp_loss: 1.452813\n","Test set: Average loss: 0.36, Accuracy: 146/150 F1 (97.33%), F1 score: 90.61%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 20.321436, loss_c: 0.000348, simclr_loss: 4.717051, grp_loss: 1.452884\n","Test set: Average loss: 0.10, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 50 lr: 0.005, loss_all: 20.326361, loss_c: 0.000044, simclr_loss: 4.718387, grp_loss: 1.452770\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 20.326740, loss_c: 0.000423, simclr_loss: 4.717855, grp_loss: 1.454896\n","Test set: Average loss: 0.13, Accuracy: 147/150 F1 (98.00%), F1 score: 100.00%\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 20.315054, loss_c: 0.000161, simclr_loss: 4.715527, grp_loss: 1.452786\n","Test set: Average loss: 0.18, Accuracy: 146/150 F1 (97.33%), F1 score: 95.51%\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 20.330202, loss_c: 0.003327, simclr_loss: 4.717644, grp_loss: 1.456299\n","Test set: Average loss: 0.25, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 90 lr: 0.00024471741852423234, loss_all: 20.313734, loss_c: 0.000129, simclr_loss: 4.714946, grp_loss: 1.453819\n","Test set: Average loss: 0.20, Accuracy: 147/150 F1 (98.00%), F1 score: 95.41%\n","Test set: Average loss: 0.23, Accuracy: 147/150 F1 (98.00%), F1 score: 95.45%\n","Ep: 0 lr: 0.01, loss_all: 18.871004, loss_c: 0.000345, simclr_loss: 4.717665, grp_loss: 0.000000\n","Ep: 10 lr: 0.009755282581475769, loss_all: 19.836733, loss_c: 0.000445, simclr_loss: 4.716600, grp_loss: 0.969888\n","Test set: Average loss: 0.31, Accuracy: 142/150 F1 (94.67%), F1 score: 95.54%\n","Model saved at step 10 with best accuracy 94.66666666666667\n","Ep: 20 lr: 0.009045084971874737, loss_all: 19.973528, loss_c: 0.126284, simclr_loss: 4.718732, grp_loss: 0.972317\n","Test set: Average loss: 0.19, Accuracy: 145/150 F1 (96.67%), F1 score: 95.32%\n","Model saved at step 20 with best accuracy 96.66666666666667\n","Ep: 30 lr: 0.007938926261462366, loss_all: 19.830925, loss_c: 0.000754, simclr_loss: 4.714973, grp_loss: 0.970279\n","Test set: Average loss: 0.12, Accuracy: 144/150 F1 (96.00%), F1 score: 100.00%\n","Ep: 40 lr: 0.006545084971874737, loss_all: 19.860044, loss_c: 0.001286, simclr_loss: 4.722217, grp_loss: 0.969893\n","Test set: Average loss: 0.16, Accuracy: 145/150 F1 (96.67%), F1 score: 95.40%\n","Model saved at step 40 with best accuracy 96.66666666666667\n","Ep: 50 lr: 0.005, loss_all: 19.871574, loss_c: 0.023648, simclr_loss: 4.717965, grp_loss: 0.976067\n","Test set: Average loss: 0.21, Accuracy: 145/150 F1 (96.67%), F1 score: 90.80%\n","Model saved at step 50 with best accuracy 96.66666666666667\n","Ep: 60 lr: 0.0034549150281252645, loss_all: 19.848553, loss_c: 0.004595, simclr_loss: 4.717935, grp_loss: 0.972219\n","Test set: Average loss: 0.12, Accuracy: 145/150 F1 (96.67%), F1 score: 95.40%\n","Model saved at step 60 with best accuracy 96.66666666666667\n","Ep: 70 lr: 0.0020610737385376348, loss_all: 19.847845, loss_c: 0.005348, simclr_loss: 4.718123, grp_loss: 0.970004\n","Test set: Average loss: 0.12, Accuracy: 148/150 F1 (98.67%), F1 score: 95.45%\n","Model saved at step 70 with best accuracy 98.66666666666667\n","Ep: 80 lr: 0.0009549150281252633, loss_all: 19.842653, loss_c: 0.000037, simclr_loss: 4.717547, grp_loss: 0.972427\n","Test set: Average loss: 0.10, Accuracy: 148/150 F1 (98.67%), F1 score: 95.45%\n","Model saved at step 80 with best accuracy 98.66666666666667\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":66.111703,"end_time":"2022-08-23T17:40:55.963410","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2022-08-23T17:39:49.851707","version":"2.3.4"}},"nbformat":4,"nbformat_minor":5}